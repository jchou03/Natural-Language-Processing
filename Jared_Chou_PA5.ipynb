{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jchou03/Natural-Language-Processing/blob/main/Jared_Chou_PA5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Programming Assignment Five: Spam detection with neural network.\n",
        "\n",
        "In this assignment, you are asked to build a neural network that can detect spam from a given SMS message.\n",
        "\n",
        "The provided files are:\n",
        "1. `spam_train.csv`: a csv file containing the training data, where the 'text' column provides the sms messages and the 'label' column indicates whether the sms message is a 'ham' (0) or a 'spam' (1).\n",
        "2. `spam_test.csv`: a csv file containing the testing data, following the same format as `spam_train.csv`."
      ],
      "metadata": {
        "id": "AQ4bfw3mERtM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Compute the SMS message vector based on the average value of the word vectors that belong to the words in it.** \n",
        "\n",
        "Just like the last assignment, we compute the 'representation' of each message, i.e., the vector, by averaging word vectors with Word2Vec. But this time, we are using pre-trained [Glove word embeddings](https://nlp.stanford.edu/projects/glove/) instead. Specifically, we are using word embedding `glove.6B.100d` to obtain word vectors of each message, as long as the word is in the 'glove.6B.100d' embedding space.\n",
        "\n",
        "In other words, you need to:\n",
        "1. Have a [basic idea](https://nlp.stanford.edu/pubs/glove.pdf) of how Glove provides pre-trained word embeddings (vectors).\n",
        "2. Download and extract word vectors from `glove.6B.100d`, contained in `glove.6B.zip`.\n",
        "3. Compute the message vectors by averaging the vectors of words in the message."
      ],
      "metadata": {
        "id": "bDfTeToXFk9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/Sophomore(22-23)/CS505/CS505_Data/PA5/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgQX1r3cDZld",
        "outputId": "08d23437-fc7b-4b68-c3df-78093acdba23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "b9LV9mfgxIeY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3bc86d1-15b4-410d-8dbb-c880178a27aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert glove.6B.100d into word vectors \n",
        "import numpy as np\n",
        "def get_line_dic(dir):\n",
        "  with open(dir) as f: \n",
        "    lines = f.readlines()\n",
        "    word_vecs = {}\n",
        "    for line in lines:\n",
        "      line_words = line.split()\n",
        "      word_vecs[line_words[0]] = np.asarray([float(val) for val in line_words[1:]])\n",
        "    return word_vecs\n",
        "\n",
        "word_vecs = get_line_dic(data_path + 'glove.6B.100d.txt')\n",
        "print(\"word vectors loaded\")\n",
        "# for vec in word_vecs:\n",
        "#   print(vec)\n",
        "# convert "
      ],
      "metadata": {
        "id": "E2PI--Fq1lqD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd23d938-2933-4948-d035-f6ddfdaa887a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word vectors loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.disable_pipe(\"parser\")\n",
        "nlp.add_pipe(\"sentencizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLNr683TQC17",
        "outputId": "5e091af7-38a9-4aa8-f000-93a89ed8da4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.pipeline.sentencizer.Sentencizer at 0x7f467d3899b0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the average for each word \n",
        "def avg_word_vec (sent): \n",
        "  doc = nlp(sent)\n",
        "  avg_vec = np.asarray([0.0] * 100)\n",
        "  count = 0\n",
        "  for sent in doc.sents:\n",
        "    for word in sent:\n",
        "      w = str(word).lower()\n",
        "      if (w in word_vecs.keys()): \n",
        "        avg_vec += word_vecs[w]\n",
        "        count += 1\n",
        "  if count != 0:\n",
        "    for i,element in enumerate(avg_vec):\n",
        "      avg_vec[i] = avg_vec[i]/count\n",
        "  return avg_vec\n",
        "\n",
        "def load_from_csv (filepath):\n",
        "  df = pd.read_csv(filepath)\n",
        "  # print(df)\n",
        "  x = df['text']\n",
        "  y = df['label']\n",
        "  return x, y\n",
        "\n",
        "def avg_vecs (texts):\n",
        "  vecs = []\n",
        "  for text in texts:\n",
        "    vecs.append(avg_word_vec(text))\n",
        "  return vecs\n"
      ],
      "metadata": {
        "id": "yOmgTGNe6UT9"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load training and testing data\n",
        "train_x, train_y = load_from_csv(data_path + 'spam_train.csv')\n",
        "test_x, test_y = load_from_csv(data_path + 'spam_test.csv')\n",
        "\n",
        "# get average word vectors of all the training data\n",
        "avg_train_x = avg_vecs(train_x) \n",
        "avg_test_x = avg_vecs(test_x)"
      ],
      "metadata": {
        "id": "nJ3mlqqQ-oW0"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((avg_train_x[0]))"
      ],
      "metadata": {
        "id": "SXwM6roB74Vx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8a2973c-2bfe-4693-9028-e604d779a3a3"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.1969375   0.40137155  0.36865167 -0.20718825 -0.02065567  0.07757833\n",
            "  0.03927933 -0.067465    0.441879    0.15917017  0.139775   -0.00559917\n",
            "  0.288385    0.03816767 -0.19067117 -0.42057867  0.21766667  0.0576755\n",
            "  0.00264867  0.21329167  0.143376    0.18265467  0.1512345  -0.14424208\n",
            "  0.334619   -0.14618367 -0.19254992 -0.42458833 -0.1299887  -0.09602917\n",
            "  0.01157183  0.34134167 -0.010272    0.023002    0.08147567  0.31745417\n",
            "  0.07393383  0.373553    0.04445333 -0.22570883 -0.37323983 -0.25216667\n",
            "  0.16185033 -0.38428667  0.04887733  0.019444    0.48043    -0.15121267\n",
            " -0.23660833 -0.343635    0.01208188 -0.16245167 -0.0275325   1.13224833\n",
            " -0.06781817 -2.42701667 -0.1387265  -0.27708217  1.45199667  0.60371333\n",
            " -0.1404895   0.68789    -0.07098717  0.24411817  0.809665    0.04799167\n",
            "  0.42990333  0.05737     0.5053695  -0.02417683 -0.01987683 -0.1946005\n",
            "  0.0938095  -0.18074183  0.27892     0.0567721  -0.10301317 -0.12640867\n",
            " -0.77742333  0.18287667  0.62576667 -0.12405117 -0.36545667  0.0964791\n",
            " -1.11409833 -0.14106172 -0.085187   -0.36867    -0.1723045  -0.413705\n",
            " -0.230667   -0.14835883 -0.11752167 -0.03439998 -0.60972667  0.22702383\n",
            " -0.20334933 -0.33746833  0.64493333  0.17533083]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Build 'dataset + data loader' that can feed data to train your model with Pytorch.**\n",
        "\n",
        "Our goal is to train a spam detection model (classification). Here's an [example](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) of how a classfier is trained. Although it is for image classification, the idea is very similar:\n",
        "\n",
        "1. Prepare/build a dataset and load it with data loader;\n",
        "2. Prepare/build a model that takes the data input and predicts; and \n",
        "3. Prepare/build the optimizer and loss functions to train the model with the dataset.\n",
        "\n",
        "Naturally, the next thing to do is to prepare the data. We do it by building the 'Dataset' and 'Dataloader' with Pytorch.\n",
        "\n",
        "You may refer to [this page](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) to get an idea of how to make 'Dataset' and 'Dataloader'. \n",
        "\n",
        "Hints:\n",
        "1. Make sure `__init__` , `__len__` and `__getitem__` of your defined dataset is implemented properly. In particular, the `__getitem__` function should return the specified message vector and its label.\n",
        "2. Don't compute the message vector when calling the `__getitem__` function, otherwise the training process will slow down A LOT.\n",
        "3. Make sure the shuffle is on for your data loader setup, as the data in the csv file is not. \n",
        "\n"
      ],
      "metadata": {
        "id": "X5FMwkD-KicD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare & build dataset\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "      self.data = data\n",
        "      self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        return input, label\n",
        "\n",
        "dataset = CustomImageDataset(avg_train_x, train_y)\n",
        "data_test = CustomImageDataset(avg_test_x, test_y)\n",
        "# print(dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaHTkyzYcNZa",
        "outputId": "74b8e374-155b-4b32-c7b3-39b0b474d95a"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([-0.1969375 ,  0.40137155,  0.36865167, -0.20718825, -0.02065567,\n",
            "        0.07757833,  0.03927933, -0.067465  ,  0.441879  ,  0.15917017,\n",
            "        0.139775  , -0.00559917,  0.288385  ,  0.03816767, -0.19067117,\n",
            "       -0.42057867,  0.21766667,  0.0576755 ,  0.00264867,  0.21329167,\n",
            "        0.143376  ,  0.18265467,  0.1512345 , -0.14424208,  0.334619  ,\n",
            "       -0.14618367, -0.19254992, -0.42458833, -0.1299887 , -0.09602917,\n",
            "        0.01157183,  0.34134167, -0.010272  ,  0.023002  ,  0.08147567,\n",
            "        0.31745417,  0.07393383,  0.373553  ,  0.04445333, -0.22570883,\n",
            "       -0.37323983, -0.25216667,  0.16185033, -0.38428667,  0.04887733,\n",
            "        0.019444  ,  0.48043   , -0.15121267, -0.23660833, -0.343635  ,\n",
            "        0.01208188, -0.16245167, -0.0275325 ,  1.13224833, -0.06781817,\n",
            "       -2.42701667, -0.1387265 , -0.27708217,  1.45199667,  0.60371333,\n",
            "       -0.1404895 ,  0.68789   , -0.07098717,  0.24411817,  0.809665  ,\n",
            "        0.04799167,  0.42990333,  0.05737   ,  0.5053695 , -0.02417683,\n",
            "       -0.01987683, -0.1946005 ,  0.0938095 , -0.18074183,  0.27892   ,\n",
            "        0.0567721 , -0.10301317, -0.12640867, -0.77742333,  0.18287667,\n",
            "        0.62576667, -0.12405117, -0.36545667,  0.0964791 , -1.11409833,\n",
            "       -0.14106172, -0.085187  , -0.36867   , -0.1723045 , -0.413705  ,\n",
            "       -0.230667  , -0.14835883, -0.11752167, -0.03439998, -0.60972667,\n",
            "        0.22702383, -0.20334933, -0.33746833,  0.64493333,  0.17533083]), 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  load dataset into data loader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(data_test, batch_size=64, shuffle=True)\n",
        "\n",
        "# print(train_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmdRFSDSinmF",
        "outputId": "28c4a276-ae82-47fa-c145-90fa82712fa8"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7f467bf1ba90>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Build the neural net model.** \n",
        "\n",
        "Once the data is ready, we need to design and implement our neural network model.\n",
        "\n",
        "You should look [here](https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html) to see how a model can be defined.\n",
        "\n",
        "The model does not need to be complicated. An example structure could be:\n",
        "\n",
        "1. linear layer 100 x 15\n",
        "2. ReLU activation layer\n",
        "3. linear layer 15 x 2 (think about why here is 2 instead of 1?)\n",
        "4. Softmax activation layer\n",
        "\n",
        "But feel free to test out other possible combinations of linear layers & activation functions and whether they make significant difference to the model performance later."
      ],
      "metadata": {
        "id": "-Dav_HatOh8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class TinyModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(TinyModel, self).__init__()\n",
        "\n",
        "        self.linear1 = torch.nn.Linear(100, 200)\n",
        "        self.activation = torch.nn.ReLU()\n",
        "        self.linear2 = torch.nn.Linear(200, 2)\n",
        "        self.softmax = torch.nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "      \n",
        "model = TinyModel()\n",
        "\n",
        "# print('The model:')\n",
        "# print(model)\n",
        "\n",
        "# print('\\n\\nJust one layer:')\n",
        "# print(model.linear2)\n",
        "\n",
        "# print('\\n\\nModel params:')\n",
        "# for param in model.parameters():\n",
        "#     print(param)\n",
        "\n",
        "# print('\\n\\nLayer params:')\n",
        "# for param in model.linear2.parameters():\n",
        "#     print(param)"
      ],
      "metadata": {
        "id": "PRpeQ6O4vz7W"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Train the model with optimizer and loss function.**\n",
        "\n",
        "Lastly, we need to set up the [optimizer](https://pytorch.org/docs/stable/optim.html) and [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions) to train the model. You may refer to the links for more details. Specifically, we need Stochastic Gradient Descent (SGD) for optimizer and CrossEntropyLoss for loss function.\n",
        "\n",
        "The last thing to do is to train the model for several epochs and evaluate its performance from time to time. For example,  train the model 5000 epochs, evaluating the model every 100 epochs. If you are not sure how the training works, you may refer to the [classification model tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) to see how it is typically done. Don't forget to print the average loss of the epoch to see if the model is being optimized properly.\n",
        "\n",
        "The evaluation metric should be the [**accuracy**](https://en.wikipedia.org/wiki/Confusion_matrix) of predicting ham/spam on the testing data (TP+TN/(TP+TN+FP+FN)). The highest accuracy should be above at least **90%**. Try different settings of model structure, learning rate, and the number of training epochs  to achieve that level of accuracy."
      ],
      "metadata": {
        "id": "PkDTq-i9P4M_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "228vt_6H5oYi"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation function\n",
        "\n",
        "def eval():\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "  with torch.no_grad():\n",
        "      for data in train_dataloader:\n",
        "          inputs, labels = data\n",
        "          # calculate outputs by running images through the network\n",
        "          outputs = model(inputs)\n",
        "          # the class with the highest energy is what we choose as prediction\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "  print(\"correct: \" + str(correct) + \" total: \" + str(total))\n",
        "  print(f'Accuracy this epoch: {100 * correct / total} %')"
      ],
      "metadata": {
        "id": "479nsOtn8EXS"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "for epoch in range(5000):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(test_dataloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        # print(inputs)\n",
        "        # print(type(inputs))\n",
        "        # print(type(inputs[0][0]))\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        model.double()\n",
        "        outputs = model(inputs)\n",
        "        # print(\"outputs: \" + str(outputs))\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "    # if epoch % 2000 == 1999:    # print every 2000 mini-batches\n",
        "    #     print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "    #     running_loss = 0.0\n",
        "    #     eval()\n",
        "    if epoch % 100 == 0:\n",
        "      print(\"epoch: \" + str(epoch))\n",
        "      print(\"average loss this epoch: \" + str(running_loss / train_dataloader.__len__()))\n",
        "      eval()\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "py-sOWDk5lzW",
        "outputId": "b74fac01-0256-415b-ff26-a06dfcf6efdf"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\n",
            "average loss this epoch: 0.34700796826482055\n",
            "correct: 487 total: 1000\n",
            "Accuracy this epoch: 48.7 %\n",
            "epoch: 100\n",
            "average loss this epoch: 0.2885680533082038\n",
            "correct: 872 total: 1000\n",
            "Accuracy this epoch: 87.2 %\n",
            "epoch: 200\n",
            "average loss this epoch: 0.24134272489325467\n",
            "correct: 890 total: 1000\n",
            "Accuracy this epoch: 89.0 %\n",
            "epoch: 300\n",
            "average loss this epoch: 0.2232059320060829\n",
            "correct: 898 total: 1000\n",
            "Accuracy this epoch: 89.8 %\n",
            "epoch: 400\n",
            "average loss this epoch: 0.21510880292810247\n",
            "correct: 903 total: 1000\n",
            "Accuracy this epoch: 90.3 %\n",
            "epoch: 500\n",
            "average loss this epoch: 0.20927601852885472\n",
            "correct: 907 total: 1000\n",
            "Accuracy this epoch: 90.7 %\n",
            "epoch: 600\n",
            "average loss this epoch: 0.20475424712423695\n",
            "correct: 910 total: 1000\n",
            "Accuracy this epoch: 91.0 %\n",
            "epoch: 700\n",
            "average loss this epoch: 0.2023045477891965\n",
            "correct: 911 total: 1000\n",
            "Accuracy this epoch: 91.1 %\n",
            "epoch: 800\n",
            "average loss this epoch: 0.19966670406509462\n",
            "correct: 914 total: 1000\n",
            "Accuracy this epoch: 91.4 %\n",
            "epoch: 900\n",
            "average loss this epoch: 0.1983384887178037\n",
            "correct: 915 total: 1000\n",
            "Accuracy this epoch: 91.5 %\n",
            "epoch: 1000\n",
            "average loss this epoch: 0.19613682470333238\n",
            "correct: 916 total: 1000\n",
            "Accuracy this epoch: 91.6 %\n",
            "epoch: 1100\n",
            "average loss this epoch: 0.19508318305566155\n",
            "correct: 918 total: 1000\n",
            "Accuracy this epoch: 91.8 %\n",
            "epoch: 1200\n",
            "average loss this epoch: 0.19323301675902882\n",
            "correct: 917 total: 1000\n",
            "Accuracy this epoch: 91.7 %\n",
            "epoch: 1300\n",
            "average loss this epoch: 0.1920283433666604\n",
            "correct: 918 total: 1000\n",
            "Accuracy this epoch: 91.8 %\n",
            "epoch: 1400\n",
            "average loss this epoch: 0.1907701610655398\n",
            "correct: 918 total: 1000\n",
            "Accuracy this epoch: 91.8 %\n",
            "epoch: 1500\n",
            "average loss this epoch: 0.1892955190293293\n",
            "correct: 919 total: 1000\n",
            "Accuracy this epoch: 91.9 %\n",
            "epoch: 1600\n",
            "average loss this epoch: 0.18790207053656874\n",
            "correct: 920 total: 1000\n",
            "Accuracy this epoch: 92.0 %\n",
            "epoch: 1700\n",
            "average loss this epoch: 0.1877140201375845\n",
            "correct: 920 total: 1000\n",
            "Accuracy this epoch: 92.0 %\n",
            "epoch: 1800\n",
            "average loss this epoch: 0.18638259953884867\n",
            "correct: 920 total: 1000\n",
            "Accuracy this epoch: 92.0 %\n",
            "epoch: 1900\n",
            "average loss this epoch: 0.18649784380333873\n",
            "correct: 920 total: 1000\n",
            "Accuracy this epoch: 92.0 %\n",
            "epoch: 2000\n",
            "average loss this epoch: 0.18551363192502698\n",
            "correct: 922 total: 1000\n",
            "Accuracy this epoch: 92.2 %\n",
            "epoch: 2100\n",
            "average loss this epoch: 0.18449185511768085\n",
            "correct: 922 total: 1000\n",
            "Accuracy this epoch: 92.2 %\n",
            "epoch: 2200\n",
            "average loss this epoch: 0.1838760230629997\n",
            "correct: 923 total: 1000\n",
            "Accuracy this epoch: 92.3 %\n",
            "epoch: 2300\n",
            "average loss this epoch: 0.18320002557627074\n",
            "correct: 923 total: 1000\n",
            "Accuracy this epoch: 92.3 %\n",
            "epoch: 2400\n",
            "average loss this epoch: 0.18345677156006618\n",
            "correct: 925 total: 1000\n",
            "Accuracy this epoch: 92.5 %\n",
            "epoch: 2500\n",
            "average loss this epoch: 0.18229621971058566\n",
            "correct: 926 total: 1000\n",
            "Accuracy this epoch: 92.6 %\n",
            "epoch: 2600\n",
            "average loss this epoch: 0.1817326920561475\n",
            "correct: 926 total: 1000\n",
            "Accuracy this epoch: 92.6 %\n",
            "epoch: 2700\n",
            "average loss this epoch: 0.18166931272250672\n",
            "correct: 926 total: 1000\n",
            "Accuracy this epoch: 92.6 %\n",
            "epoch: 2800\n",
            "average loss this epoch: 0.18083871233668838\n",
            "correct: 925 total: 1000\n",
            "Accuracy this epoch: 92.5 %\n",
            "epoch: 2900\n",
            "average loss this epoch: 0.18041703860906255\n",
            "correct: 925 total: 1000\n",
            "Accuracy this epoch: 92.5 %\n",
            "epoch: 3000\n",
            "average loss this epoch: 0.18089670874357677\n",
            "correct: 926 total: 1000\n",
            "Accuracy this epoch: 92.6 %\n",
            "epoch: 3100\n",
            "average loss this epoch: 0.17947416858984683\n",
            "correct: 928 total: 1000\n",
            "Accuracy this epoch: 92.8 %\n",
            "epoch: 3200\n",
            "average loss this epoch: 0.17950706721637577\n",
            "correct: 927 total: 1000\n",
            "Accuracy this epoch: 92.7 %\n",
            "epoch: 3300\n",
            "average loss this epoch: 0.17947110022527624\n",
            "correct: 928 total: 1000\n",
            "Accuracy this epoch: 92.8 %\n",
            "epoch: 3400\n",
            "average loss this epoch: 0.17871225447036088\n",
            "correct: 928 total: 1000\n",
            "Accuracy this epoch: 92.8 %\n",
            "epoch: 3500\n",
            "average loss this epoch: 0.17919281016766303\n",
            "correct: 928 total: 1000\n",
            "Accuracy this epoch: 92.8 %\n",
            "epoch: 3600\n",
            "average loss this epoch: 0.17805686404198737\n",
            "correct: 927 total: 1000\n",
            "Accuracy this epoch: 92.7 %\n",
            "epoch: 3700\n",
            "average loss this epoch: 0.17830319884127704\n",
            "correct: 928 total: 1000\n",
            "Accuracy this epoch: 92.8 %\n",
            "epoch: 3800\n",
            "average loss this epoch: 0.1776681604797365\n",
            "correct: 929 total: 1000\n",
            "Accuracy this epoch: 92.9 %\n",
            "epoch: 3900\n",
            "average loss this epoch: 0.1770776282869449\n",
            "correct: 931 total: 1000\n",
            "Accuracy this epoch: 93.1 %\n",
            "epoch: 4000\n",
            "average loss this epoch: 0.1773674901011732\n",
            "correct: 931 total: 1000\n",
            "Accuracy this epoch: 93.1 %\n",
            "epoch: 4100\n",
            "average loss this epoch: 0.17744000917065175\n",
            "correct: 930 total: 1000\n",
            "Accuracy this epoch: 93.0 %\n",
            "epoch: 4200\n",
            "average loss this epoch: 0.17687390579346302\n",
            "correct: 930 total: 1000\n",
            "Accuracy this epoch: 93.0 %\n",
            "epoch: 4300\n",
            "average loss this epoch: 0.17690188350506833\n",
            "correct: 929 total: 1000\n",
            "Accuracy this epoch: 92.9 %\n",
            "epoch: 4400\n",
            "average loss this epoch: 0.176801697717451\n",
            "correct: 928 total: 1000\n",
            "Accuracy this epoch: 92.8 %\n",
            "epoch: 4500\n",
            "average loss this epoch: 0.17604008991075565\n",
            "correct: 928 total: 1000\n",
            "Accuracy this epoch: 92.8 %\n",
            "epoch: 4600\n",
            "average loss this epoch: 0.1764229537578229\n",
            "correct: 928 total: 1000\n",
            "Accuracy this epoch: 92.8 %\n",
            "epoch: 4700\n",
            "average loss this epoch: 0.1761628076365441\n",
            "correct: 928 total: 1000\n",
            "Accuracy this epoch: 92.8 %\n",
            "epoch: 4800\n",
            "average loss this epoch: 0.17579165975754477\n",
            "correct: 928 total: 1000\n",
            "Accuracy this epoch: 92.8 %\n",
            "epoch: 4900\n",
            "average loss this epoch: 0.1758607307883459\n",
            "correct: 928 total: 1000\n",
            "Accuracy this epoch: 92.8 %\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def train():\n",
        "#   for data in train_dataloader:\n",
        "#     inputs, labels = data\n",
        "#     outputs = print(inputs.shape)\n",
        "#     print(labels.shape)\n",
        "#     break;"
      ],
      "metadata": {
        "id": "1efBSfuW4mxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your training output should look similar to this (w/ # of epoch, accuracies, average loss, etc.)"
      ],
      "metadata": {
        "id": "3DryfKzaKhuU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ff13f0e-05f1-4f4a-ff71-9bc1f02d7c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1\n",
            "average loss this epoch: 0.6933\n",
            "accuracy this epoch: 0.47\n",
            "epoch: 101\n",
            "average loss this epoch: 0.6864\n",
            "accuracy this epoch: 0.62\n",
            "epoch: 201\n",
            "average loss this epoch: 0.6784\n",
            "accuracy this epoch: 0.75\n",
            "epoch: 301\n",
            "average loss this epoch: 0.6655\n",
            "accuracy this epoch: 0.82\n",
            "epoch: 401\n",
            "average loss this epoch: 0.6497\n",
            "accuracy this epoch: 0.86\n",
            "epoch: 501\n",
            "average loss this epoch: 0.6286\n",
            "accuracy this epoch: 0.86\n",
            "epoch: 601\n",
            "average loss this epoch: 0.6030\n",
            "accuracy this epoch: 0.86\n",
            "epoch: 701\n",
            "average loss this epoch: 0.5747\n",
            "accuracy this epoch: 0.86\n",
            "epoch: 801\n",
            "average loss this epoch: 0.5486\n",
            "accuracy this epoch: 0.86\n",
            "epoch: 901\n",
            "average loss this epoch: 0.5273\n",
            "accuracy this epoch: 0.86\n",
            "epoch: 1001\n",
            "average loss this epoch: 0.5078\n",
            "accuracy this epoch: 0.87\n",
            "epoch: 1101\n",
            "average loss this epoch: 0.4917\n",
            "accuracy this epoch: 0.87\n",
            "epoch: 1201\n",
            "average loss this epoch: 0.4804\n",
            "accuracy this epoch: 0.87\n",
            "epoch: 1301\n",
            "average loss this epoch: 0.4702\n",
            "accuracy this epoch: 0.88\n",
            "epoch: 1401\n",
            "average loss this epoch: 0.4630\n",
            "accuracy this epoch: 0.88\n",
            "epoch: 1501\n",
            "average loss this epoch: 0.4562\n",
            "accuracy this epoch: 0.89\n",
            "epoch: 1601\n",
            "average loss this epoch: 0.4505\n",
            "accuracy this epoch: 0.88\n",
            "epoch: 1701\n",
            "average loss this epoch: 0.4454\n",
            "accuracy this epoch: 0.88\n",
            "epoch: 1801\n",
            "average loss this epoch: 0.4422\n",
            "accuracy this epoch: 0.89\n",
            "epoch: 1901\n",
            "average loss this epoch: 0.4375\n",
            "accuracy this epoch: 0.89\n",
            "epoch: 2001\n",
            "average loss this epoch: 0.4344\n",
            "accuracy this epoch: 0.89\n",
            "epoch: 2101\n",
            "average loss this epoch: 0.4311\n",
            "accuracy this epoch: 0.89\n",
            "epoch: 2201\n",
            "average loss this epoch: 0.4281\n",
            "accuracy this epoch: 0.90\n",
            "epoch: 2301\n",
            "average loss this epoch: 0.4250\n",
            "accuracy this epoch: 0.90\n",
            "epoch: 2401\n",
            "average loss this epoch: 0.4242\n",
            "accuracy this epoch: 0.90\n",
            "epoch: 2501\n",
            "average loss this epoch: 0.4214\n",
            "accuracy this epoch: 0.90\n",
            "epoch: 2601\n",
            "average loss this epoch: 0.4187\n",
            "accuracy this epoch: 0.90\n",
            "epoch: 2701\n",
            "average loss this epoch: 0.4167\n",
            "accuracy this epoch: 0.90\n",
            "epoch: 2801\n",
            "average loss this epoch: 0.4155\n",
            "accuracy this epoch: 0.90\n",
            "epoch: 2901\n",
            "average loss this epoch: 0.4136\n",
            "accuracy this epoch: 0.90\n",
            "epoch: 3001\n",
            "average loss this epoch: 0.4119\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 3101\n",
            "average loss this epoch: 0.4097\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 3201\n",
            "average loss this epoch: 0.4095\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 3301\n",
            "average loss this epoch: 0.4076\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 3401\n",
            "average loss this epoch: 0.4069\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 3501\n",
            "average loss this epoch: 0.4053\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 3601\n",
            "average loss this epoch: 0.4034\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 3701\n",
            "average loss this epoch: 0.4032\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 3801\n",
            "average loss this epoch: 0.4012\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 3901\n",
            "average loss this epoch: 0.4016\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 4001\n",
            "average loss this epoch: 0.3998\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 4101\n",
            "average loss this epoch: 0.3983\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 4201\n",
            "average loss this epoch: 0.3969\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 4301\n",
            "average loss this epoch: 0.3960\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 4401\n",
            "average loss this epoch: 0.3972\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 4501\n",
            "average loss this epoch: 0.3953\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 4601\n",
            "average loss this epoch: 0.3930\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 4701\n",
            "average loss this epoch: 0.3939\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 4801\n",
            "average loss this epoch: 0.3929\n",
            "accuracy this epoch: 0.92\n",
            "epoch: 4901\n",
            "average loss this epoch: 0.3918\n",
            "accuracy this epoch: 0.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bk61zTxvUj7K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}