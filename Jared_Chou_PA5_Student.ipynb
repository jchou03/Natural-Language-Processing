{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jchou03/Natural-Language-Processing/blob/main/Jared_Chou_PA5_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Programming Assignment Five: Spam detection with neural network.\n",
        "\n",
        "In this assignment, you are asked to build a neural network that can detect spam from a given SMS message.\n",
        "\n",
        "The provided files are:\n",
        "1. `spam_train.csv`: a csv file containing the training data, where the 'text' column provides the sms messages and the 'label' column indicates whether the sms message is a 'ham' (0) or a 'spam' (1).\n",
        "2. `spam_test.csv`: a csv file containing the testing data, following the same format as `spam_train.csv`."
      ],
      "metadata": {
        "id": "AQ4bfw3mERtM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Compute the SMS message vector based on the average value of the word vectors that belong to the words in it.** \n",
        "\n",
        "Just like the last assignment, we compute the 'representation' of each message, i.e., the vector, by averaging word vectors with Word2Vec. But this time, we are using pre-trained [Glove word embeddings](https://nlp.stanford.edu/projects/glove/) instead. Specifically, we are using word embedding `glove.6B.100d` to obtain word vectors of each message, as long as the word is in the 'glove.6B.100d' embedding space.\n",
        "\n",
        "In other words, you need to:\n",
        "1. Have a [basic idea](https://nlp.stanford.edu/pubs/glove.pdf) of how Glove provides pre-trained word embeddings (vectors).\n",
        "2. Download and extract word vectors from `glove.6B.100d`, contained in `glove.6B.zip`.\n",
        "3. Compute the message vectors by averaging the vectors of words in the message."
      ],
      "metadata": {
        "id": "bDfTeToXFk9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "b9LV9mfgxIeY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b3f5615-afce-4dd9-bfe9-03875c830cfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert glove.6B.100d into word vectors \n",
        "import numpy as np\n",
        "def get_line_dic():\n",
        "  with open('glove.6B.100d.txt') as f: \n",
        "    lines = f.readlines()\n",
        "    word_vecs = {}\n",
        "    for line in lines:\n",
        "      line_words = line.split()\n",
        "      word_vecs[line_words[0]] = np.asarray([float(val) for val in line_words[1:]])\n",
        "    return word_vecs\n",
        "\n",
        "word_vecs = get_line_dic()\n",
        "print(\"word vectors loaded\")\n",
        "# for vec in word_vecs:\n",
        "#   print(vec)\n",
        "# convert "
      ],
      "metadata": {
        "id": "E2PI--Fq1lqD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f2978a0-1e7d-4b19-f882-671a34462634"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word vectors loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.disable_pipe(\"parser\")\n",
        "nlp.add_pipe(\"sentencizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLNr683TQC17",
        "outputId": "eaadd036-d5fe-4769-b4b7-d9287886a159"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.pipeline.sentencizer.Sentencizer at 0x7f987c555190>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the average for each word \n",
        "def avg_word_vec (sent): \n",
        "  doc = nlp(sent)\n",
        "  avg_vec = np.asarray([0.0] * 100)\n",
        "  count = 0\n",
        "  for sent in doc.sents:\n",
        "    for word in sent:\n",
        "      w = str(word).lower()\n",
        "      if (w in word_vecs.keys()): \n",
        "        avg_vec += word_vecs[w]\n",
        "        count += 1\n",
        "  if count != 0:\n",
        "    for i,element in enumerate(avg_vec):\n",
        "      avg_vec[i] = avg_vec[i]/count\n",
        "  return avg_vec\n",
        "\n",
        "def load_from_csv (filepath):\n",
        "  df = pd.read_csv(filepath)\n",
        "  # print(df)\n",
        "  x = df['text']\n",
        "  y = df['label']\n",
        "  return x, y\n",
        "\n",
        "def avg_vecs (texts):\n",
        "  vecs = []\n",
        "  for text in texts:\n",
        "    vecs.append(avg_word_vec(text))\n"
      ],
      "metadata": {
        "id": "yOmgTGNe6UT9"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, train_y = load_from_csv('spam_train.csv')\n",
        "test_x, test_y = load_from_csv('spam_test.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "nJ3mlqqQ-oW0"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SXwM6roB74Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Build 'dataset + data loader' that can feed data to train your model with Pytorch.**\n",
        "\n",
        "Our goal is to train a spam detection model (classification). Here's an [example](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) of how a classfier is trained. Although it is for image classification, the idea is very similar:\n",
        "\n",
        "1. Prepare/build a dataset and load it with data loader;\n",
        "2. Prepare/build a model that takes the data input and predicts; and \n",
        "3. Prepare/build the optimizer and loss functions to train the model with the dataset.\n",
        "\n",
        "Naturally, the next thing to do is to prepare the data. We do it by building the 'Dataset' and 'Dataloader' with Pytorch.\n",
        "\n",
        "You may refer to [this page](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) to get an idea of how to make 'Dataset' and 'Dataloader'. \n",
        "\n",
        "Hints:\n",
        "1. Make sure `__init__` , `__len__` and `__getitem__` of your defined dataset is implemented properly. In particular, the `__getitem__` function should return the specified message vector and its label.\n",
        "2. Don't compute the message vector when calling the `__getitem__` function, otherwise the training process will slow down A LOT.\n",
        "3. Make sure the shuffle is on for your data loader setup, as the data in the csv file is not. \n",
        "\n"
      ],
      "metadata": {
        "id": "X5FMwkD-KicD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Build the neural net model.** \n",
        "\n",
        "Once the data is ready, we need to design and implement our neural network model.\n",
        "\n",
        "You should look [here](https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html) to see how a model can be defined.\n",
        "\n",
        "The model does not need to be complicated. An example structure could be:\n",
        "\n",
        "1. linear layer 100 x 15\n",
        "2. ReLU activation layer\n",
        "3. linear layer 15 x 2 (think about why here is 2 instead of 1?)\n",
        "4. Softmax activation layer\n",
        "\n",
        "But feel free to test out other possible combinations of linear layers & activation functions and whether they make significant difference to the model performance later."
      ],
      "metadata": {
        "id": "-Dav_HatOh8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Train the model with optimizer and loss function.**\n",
        "\n",
        "Lastly, we need to set up the [optimizer](https://pytorch.org/docs/stable/optim.html) and [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions) to train the model. You may refer to the links for more details. Specifically, we need Stochastic Gradient Descent (SGD) for optimizer and CrossEntropyLoss for loss function.\n",
        "\n",
        "The last thing to do is to train the model for several epochs and evaluate its performance from time to time. For example,  train the model 5000 epochs, evaluating the model every 100 epochs. If you are not sure how the training works, you may refer to the [classification model tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) to see how it is typically done. Don't forget to print the average loss of the epoch to see if the model is being optimized properly.\n",
        "\n",
        "The evaluation metric should be the [**accuracy**](https://en.wikipedia.org/wiki/Confusion_matrix) of predicting ham/spam on the testing data (TP+TN/(TP+TN+FP+FN)). The highest accuracy should be above at least **90%**. Try different settings of model structure, learning rate, and the number of training epochs  to achieve that level of accuracy."
      ],
      "metadata": {
        "id": "PkDTq-i9P4M_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your training output should look similar to this (w/ # of epoch, accuracies, average loss, etc.)"
      ],
      "metadata": {
        "id": "3DryfKzaKhuU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ff13f0e-05f1-4f4a-ff71-9bc1f02d7c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1\n",
            "average loss this epoch: 0.6933\n",
            "accuracy this epoch: 0.47\n",
            "epoch: 101\n",
            "average loss this epoch: 0.6864\n",
            "accuracy this epoch: 0.62\n",
            "epoch: 201\n",
            "average loss this epoch: 0.6784\n",
            "accuracy this epoch: 0.75\n",
            "epoch: 301\n",
            "average loss this epoch: 0.6655\n",
            "accuracy this epoch: 0.82\n",
            "epoch: 401\n",
            "average loss this epoch: 0.6497\n",
            "accuracy this epoch: 0.86\n",
            "epoch: 501\n",
            "average loss this epoch: 0.6286\n",
            "accuracy this epoch: 0.86\n",
            "epoch: 601\n",
            "average loss this epoch: 0.6030\n",
            "accuracy this epoch: 0.86\n",
            "epoch: 701\n",
            "average loss this epoch: 0.5747\n",
            "accuracy this epoch: 0.86\n",
            "epoch: 801\n",
            "average loss this epoch: 0.5486\n",
            "accuracy this epoch: 0.86\n",
            "epoch: 901\n",
            "average loss this epoch: 0.5273\n",
            "accuracy this epoch: 0.86\n",
            "epoch: 1001\n",
            "average loss this epoch: 0.5078\n",
            "accuracy this epoch: 0.87\n",
            "epoch: 1101\n",
            "average loss this epoch: 0.4917\n",
            "accuracy this epoch: 0.87\n",
            "epoch: 1201\n",
            "average loss this epoch: 0.4804\n",
            "accuracy this epoch: 0.87\n",
            "epoch: 1301\n",
            "average loss this epoch: 0.4702\n",
            "accuracy this epoch: 0.88\n",
            "epoch: 1401\n",
            "average loss this epoch: 0.4630\n",
            "accuracy this epoch: 0.88\n",
            "epoch: 1501\n",
            "average loss this epoch: 0.4562\n",
            "accuracy this epoch: 0.89\n",
            "epoch: 1601\n",
            "average loss this epoch: 0.4505\n",
            "accuracy this epoch: 0.88\n",
            "epoch: 1701\n",
            "average loss this epoch: 0.4454\n",
            "accuracy this epoch: 0.88\n",
            "epoch: 1801\n",
            "average loss this epoch: 0.4422\n",
            "accuracy this epoch: 0.89\n",
            "epoch: 1901\n",
            "average loss this epoch: 0.4375\n",
            "accuracy this epoch: 0.89\n",
            "epoch: 2001\n",
            "average loss this epoch: 0.4344\n",
            "accuracy this epoch: 0.89\n",
            "epoch: 2101\n",
            "average loss this epoch: 0.4311\n",
            "accuracy this epoch: 0.89\n",
            "epoch: 2201\n",
            "average loss this epoch: 0.4281\n",
            "accuracy this epoch: 0.90\n",
            "epoch: 2301\n",
            "average loss this epoch: 0.4250\n",
            "accuracy this epoch: 0.90\n",
            "epoch: 2401\n",
            "average loss this epoch: 0.4242\n",
            "accuracy this epoch: 0.90\n",
            "epoch: 2501\n",
            "average loss this epoch: 0.4214\n",
            "accuracy this epoch: 0.90\n",
            "epoch: 2601\n",
            "average loss this epoch: 0.4187\n",
            "accuracy this epoch: 0.90\n",
            "epoch: 2701\n",
            "average loss this epoch: 0.4167\n",
            "accuracy this epoch: 0.90\n",
            "epoch: 2801\n",
            "average loss this epoch: 0.4155\n",
            "accuracy this epoch: 0.90\n",
            "epoch: 2901\n",
            "average loss this epoch: 0.4136\n",
            "accuracy this epoch: 0.90\n",
            "epoch: 3001\n",
            "average loss this epoch: 0.4119\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 3101\n",
            "average loss this epoch: 0.4097\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 3201\n",
            "average loss this epoch: 0.4095\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 3301\n",
            "average loss this epoch: 0.4076\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 3401\n",
            "average loss this epoch: 0.4069\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 3501\n",
            "average loss this epoch: 0.4053\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 3601\n",
            "average loss this epoch: 0.4034\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 3701\n",
            "average loss this epoch: 0.4032\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 3801\n",
            "average loss this epoch: 0.4012\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 3901\n",
            "average loss this epoch: 0.4016\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 4001\n",
            "average loss this epoch: 0.3998\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 4101\n",
            "average loss this epoch: 0.3983\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 4201\n",
            "average loss this epoch: 0.3969\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 4301\n",
            "average loss this epoch: 0.3960\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 4401\n",
            "average loss this epoch: 0.3972\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 4501\n",
            "average loss this epoch: 0.3953\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 4601\n",
            "average loss this epoch: 0.3930\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 4701\n",
            "average loss this epoch: 0.3939\n",
            "accuracy this epoch: 0.91\n",
            "epoch: 4801\n",
            "average loss this epoch: 0.3929\n",
            "accuracy this epoch: 0.92\n",
            "epoch: 4901\n",
            "average loss this epoch: 0.3918\n",
            "accuracy this epoch: 0.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bk61zTxvUj7K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}