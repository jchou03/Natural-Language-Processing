{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jchou03/Natural-Language-Processing/blob/main/CS505_PA2_Jared_Chou.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS 505: Programming Assignment 2\n",
        "author: Jared Chou\n",
        "\n",
        "Collaborators: Macie So\n"
      ],
      "metadata": {
        "id": "c24Aa9kubujF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 2**: (One week, due in Gradescope at midnight 9/28 with same grace period and late policy as in PS 1)\n",
        "\n",
        "In this assignment, we are going to try building language models with \n",
        "the data we collected from various sources.\n",
        "\n",
        "In the first half, we are going to analyze our twitter data with NLTK. \n",
        "There are several tasks we would like to you to finish during this process:\n",
        "\n",
        "1. Preprocess the raw twitter data and make them into a format that\n",
        "language models in NLTK can train with.\n",
        "2. Train uni-gram, bi-gram and tri-gram models with Add-one smoothing.\n",
        "3. Compute perplexity to evaluate our language models based on different test sets.\n",
        "4. Generate new sentences with our language models based on the trained data.\n",
        "5. Perform sentiment analysis on our scraped data.\n",
        "\n",
        "In the following sections, we are going to provide a code template to allow you\n",
        "to complete them step by step.\n",
        "\n",
        "Here's a [general guide](https://www.kaggle.com/code/alvations/n-gram-language-model-with-nltk/notebook) of how to build language model with NLTK, please refer to this guide from time to time to see what you missed.\n",
        "\n",
        "Please submit this code with your implementaton and outputs. **Please indicate which students, if any, you consulted with as you completed this assignment.** "
      ],
      "metadata": {
        "id": "XF8Ns8RL2_Kq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, please go back to the code of our first lab section. Scrape 10000 tweets\n",
        "which: football lang:en -has:mentions -has:links -is:retweet\n",
        "1. mentions 'fishing'\n",
        "2. is written in English\n",
        "3. does not mention any other twitter account (i.e. @).\n",
        "4. does not contain links.\n",
        "5. is not a re-tweet.\n",
        "\n",
        "Then, scrape 10000 tweets with the same rules above but mention 'football' instead of 'fishing' this time.\n",
        "\n",
        "Save the scraped tweets in separate files, one for 'fishing' tweets\n",
        "and one for 'football'."
      ],
      "metadata": {
        "id": "AIYKCaG57Hza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install tweepy\n",
        "!pip3 install tweepy\n",
        "!pip3 install tweepy --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzi-4o4lcOdR",
        "outputId": "e9425cdd-4224-4570-edb9-371a6246f1c9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.7/dist-packages (4.10.1)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (2.28.1)\n",
            "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.3.1)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (3.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy) (2022.6.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy) (2.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.7/dist-packages (4.10.1)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (2.28.1)\n",
            "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.3.1)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy) (2022.6.15)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy) (2.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports & initialize client\n",
        "import tweepy\n",
        "import csv\n",
        "\n",
        "client = tweepy.Client(bearer_token='AAAAAAAAAAAAAAAAAAAAALZxhAEAAAAANF7xDDhnPvnIzDbG1DAgwfW%2Ft%2Bo%3DqLKehartkTnY8z7aa2uYo3doESHKL9UZhtJD0gQJiI5HxpyZDK')"
      ],
      "metadata": {
        "id": "3ybVRNlYcf5s"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get tweets & create CSV\n",
        "\n",
        "query = \"football lang:en -has:links -has:mentions -is:retweet\"\n",
        "footballTweets = list(tweepy.Paginator(client.search_recent_tweets, query = query, tweet_fields=['context_annotations', 'created_at'], max_results=100).flatten(limit=10000))\n",
        "print(\"{} tweeets are collected\".format(len(footballTweets)))\n",
        "\n",
        "query = \"fishing lang:en -has:links -has:mentions -is:retweet\"\n",
        "fishingTweets = list(tweepy.Paginator(client.search_recent_tweets, query = query, tweet_fields=['context_annotations', 'created_at'], max_results=100).flatten(limit=10000))\n",
        "print(\"{} tweeets are collected\".format(len(fishingTweets)))"
      ],
      "metadata": {
        "id": "czrS-Ja1ckP3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80f7c45e-fc3b-411a-b3a9-93a33509b27c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000 tweeets are collected\n",
            "7951 tweeets are collected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save information to CSV File\n",
        "\n",
        "with open(\"footballTweets.csv\", 'w', newline='') as csvfile:\n",
        "  fieldnames = ['idx', 'tweetId', 'tweetText']\n",
        "  writer = csv.DictWriter(csvfile, fieldnames = fieldnames)\n",
        "  writer.writeheader()\n",
        "  for i, tweet in enumerate(footballTweets):\n",
        "    writer.writerow({'idx':i, 'tweetId':tweet.id, 'tweetText':tweet.data['text']})\n",
        "\n",
        "with open(\"fishingTweets.csv\", 'w', newline='') as csvfile:\n",
        "  fieldnames = ['idx', 'tweetId', 'tweetText']\n",
        "  writer = csv.DictWriter(csvfile, fieldnames = fieldnames)\n",
        "  writer.writeheader()\n",
        "  for i, tweet in enumerate(fishingTweets):\n",
        "    writer.writerow({'idx':i, 'tweetId':tweet.id, 'tweetText':tweet.data['text']})"
      ],
      "metadata": {
        "id": "2j5Jk-ZIeUOE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1\n",
        "\n",
        "First, let's try loading our scraped data. To begin with, let's load our 'fishing' data. You may change the following\n",
        "function as necessary."
      ],
      "metadata": {
        "id": "MdfD899a8s0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def loadTextFromCSV(csvPath):\n",
        "  tweetDict = {}\n",
        "  with open(csvPath, newline='') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    for row in reader:\n",
        "      tweetDict[int(row['idx'])] = row['tweetText']\n",
        "  return tweetDict\n",
        "\n",
        "#load your fishing tweet data here:\n",
        "csvPathFish = \"./fishingTweets.csv\"\n",
        "rawTweetDictFish = loadTextFromCSV(csvPathFish)\n",
        "\n",
        "#load football tweet data here:\n",
        "csvPathFootball = \"./footballTweets.csv\"\n",
        "rawTweetDictFootball = loadTextFromCSV(csvPathFootball)\n",
        "\n",
        "#print your tweet dictionary. You should see your saved tweets inside.\n",
        "# print(\"rawTweetDictFish: \",rawTweetDictFish)"
      ],
      "metadata": {
        "id": "79qabVAI6_gK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2\n",
        "\n",
        "Next, we are going to pre-process texts with NLTK library.\n",
        "\n",
        "Install NLTK library if it's not in your Google Colab space.\n",
        "\n",
        "Download 'punkt' specifically for sentence segmentation."
      ],
      "metadata": {
        "id": "P8YsEnEyDaoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "3JTVhy2Q9hjr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f195add-b9aa-491d-f234-c84a6f0cc307"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "T_SvPCIV8pLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c7d3b24-24b9-474e-f950-f8e31b159884"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We preprocess our tweet data with the following steps:\n",
        "1. Split data into training and testing splits. (**80%** tweets for training and **20%** tweets for testing) \n",
        "2. Sentence segmentation/spliting.\n",
        "3. Lower-case all words in the sentences.\n",
        "4. Tokenization (you should use TweetTokenizer from NLTK.tokenize)\n",
        "5. Padding with begin-of-sentence and end-of-sentence symbols \n",
        "\n",
        "You may refer to the following materials:\n",
        "1. [Sentence segmentation](https://www.nltk.org/api/nltk.tokenize.html). \n",
        "2. [String Lower case](https://www.w3schools.com/python/ref_string_lower.asp).\n",
        "3. [Tweet Tokenization](https://www.nltk.org/api/nltk.tokenize.casual.html).\n",
        "4. [Padding tokenized sentences](https://www.nltk.org/_modules/nltk/lm/preprocessing.html). Particularly, please look at function 'padded_everygram_pipeline'.\n",
        "\n",
        "We will handle the first 3 steps in the following block."
      ],
      "metadata": {
        "id": "FNzigJJpER3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.text import sent_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "tknzr = TweetTokenizer()\n",
        "\n",
        "# Here's a template you may want to start with for your data pre-processing.\n",
        "\n",
        "# Wenda recommends us delay padding tokenized sentences until right before model training\n",
        "\n",
        "# moved to global scope so it can be used in step 6\n",
        "def sentenceSegmentation(tweet):\n",
        "  #Input: a string of raw tweet\n",
        "  #Output: a list of strings, each element in the list is a segmented sentence\n",
        "  return sent_tokenize(tweet)\n",
        "  \n",
        "def sentenceLowerCase(sentence):\n",
        "    #Input: a string of sentence\n",
        "    #Output: a string of sentence, but all words in the sentence are lower-cased.\n",
        "    return sentence.lower()\n",
        "\n",
        "def sentenceTokenization(sentence):\n",
        "  #Input: a string of sentence\n",
        "  #Output: a list of tokens that belong to the sentence.\n",
        "  return tknzr.tokenize(sentence)\n",
        "\n",
        "def preprocess(rawTweetDataDict,ngram):\n",
        "  #Input: a dictionary contains raw tweet data scraped from Tweeter\n",
        "  #Output: two lists of tweet sentences (train/test), but each tweet sentence is\n",
        "  #     represented in the form of tokens.\n",
        "  \n",
        "  # train_sents = [] # list of sentences for training\n",
        "  # test_sents = [] # list of sentences for testing\n",
        "\n",
        "  train = [] # list to store all training sentences\n",
        "  test = [] # list of all testing sentences\n",
        "  \n",
        "  # segment each tweet into sentences\n",
        "  for i, tweet in rawTweetDataDict.items():\n",
        "    if(i < len(rawTweetDataDict) * 0.8):\n",
        "      train.extend(sentenceSegmentation(tweet))\n",
        "    else:\n",
        "      test.extend(sentenceSegmentation(tweet))    \n",
        "\n",
        "  # tokenize & make each tweet lowercase\n",
        "  i = 0\n",
        "  for sent in train:\n",
        "    train[i] = sentenceTokenization(sentenceLowerCase(sent))\n",
        "    i += 1\n",
        "  i = 0\n",
        "  for sent in test:\n",
        "    test[i] = sentenceTokenization(sentenceLowerCase(sent))\n",
        "    i += 1\n",
        "\n",
        "  \n",
        "  \n",
        "  return (train, test)"
      ],
      "metadata": {
        "id": "kRgHvtMHD4FL"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3\n",
        "\n",
        "Next, we build our n-gram model with our pre-processed data.\n",
        "First we need to pad our data with padded_everygram_pipeline. Then, we train our n-gram model with add-one smoothing using the corresponding functions in NLTK.\n",
        "\n",
        "Related materials:\n",
        "1. [Padding](https://www.nltk.org/_modules/nltk/lm/preprocessing.html)\n",
        "2. [N-gram Language Model](https://www.nltk.org/api/nltk.lm.html)\n",
        "\n",
        "Let's train unigram, bigram and trigram models with our train data split."
      ],
      "metadata": {
        "id": "8BYp6rJmMshM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here's a template you may want to start with\n",
        "\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import NgramCounter\n",
        "from nltk.util import ngrams\n",
        "from nltk.lm import Laplace\n",
        "# from nltk.lm import StupidBackoff\n",
        "\n",
        "def trainNGramAddOneSmoothing(trainData,ngram):\n",
        "  # Input: a list of tweet sentences, each element is a list of tokens; n for ngram model\n",
        "  # Output: a n-gram model with add-one smoothing trained on your input data.\n",
        "\n",
        "  train, vocab = padded_everygram_pipeline(order = ngram, text = trainData) # pad each sentence and save it to a list\n",
        "  # text_ngrams = [ngrams(sent, ngram) for sent in train] # all ngrams from the text\n",
        "  # ngram_counts = NgramCounter(text_ngrams)\n",
        "  laplace = Laplace(order = ngram) # laplace add-one smoothing implementation model\n",
        "  laplace.fit(train, vocab)\n",
        "  return laplace\n",
        "\n",
        "  # model = StupidBackoff(order = 3)\n",
        "  # model.fit(train, vocab)\n"
      ],
      "metadata": {
        "id": "sQrdUMTJN-6z"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4\n",
        "\n",
        "Now we apply our analysis on the trained model. \n",
        "\n",
        "First, compute the average perplexity of your tri-gram model on the sentences of our test data.\n",
        "\n",
        "[How to compute perplexity](https://www.nltk.org/api/nltk.lm.html)\n",
        "\n",
        "Next, load the tweet data of 'football' instead, and compute the perplexity of your 'fishing' model on the football tweets. \n",
        "\n",
        "**Why is there a difference between the two perplexities, what causes it?**\n",
        "(Please answer in a text cell.)"
      ],
      "metadata": {
        "id": "HoxnRMdxOnSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here's a template you may want to start with\n",
        "from nltk.lm import MLE\n",
        "\n",
        "def computePerplexity(model,testData):\n",
        "  # Input: your model; the testing data\n",
        "  # Output: average perplexity of the model on your testing data.\n",
        "  return model.perplexity(testData)\n",
        "\n",
        "\n",
        "# compute \n",
        "# 1. the perplexity of your model on your testing data of 'fishing' tweets.\n",
        "fishTrain, fishTest = preprocess(rawTweetDictFish, 3)\n",
        "fish_lm = trainNGramAddOneSmoothing(fishTrain, 3)\n",
        "fish_p = computePerplexity(fish_lm, fishTest)\n",
        "print(fish_p)\n",
        "# 2. the perplexity of your model on your data of 'football' tweets.\n",
        "footballTrain, footballTest = preprocess(rawTweetDictFootball, 3)\n",
        "football_lm = trainNGramAddOneSmoothing(footballTrain, 3)\n",
        "football_p = computePerplexity(football_lm, footballTest)\n",
        "print(football_p)"
      ],
      "metadata": {
        "id": "S9S4GvSjP_Nt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cc0b166-479e-4f03-e3c9-e386159bae33"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14245.614989268004\n",
            "17781.34314640041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5\n",
        "\n",
        "Next, generate 10 tweets using each of your language models (unigram, bigram, trigram). The generated tweets needs to be in string format instead of tokens, also the string should be without padding.\n",
        "\n",
        "[Generate new sentences with your model.](https://www.nltk.org/api/nltk.lm.html)\n",
        "\n",
        "[Detokenize your generated sentences](https://www.nltk.org/howto/tokenize.html)"
      ],
      "metadata": {
        "id": "qk9I-CruVbHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here's a template you may want to start with\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "import random\n",
        "\n",
        "def generateNewSentence(model,randomSeed):\n",
        "  # Input: your model; random seed that get you different generated sentence\n",
        "  # Output: a new sentence generated by your model, but in a string format instead of tokens.\n",
        "  detokenizer = TreebankWordDetokenizer()\n",
        "  gen_tweet = model.generate(10)\n",
        "  tweet = []\n",
        "  # remove the starting and ending sentence words\n",
        "  for word in gen_tweet:\n",
        "    if word != \"</s>\" and word != \"<s>\":\n",
        "      tweet.append(word)\n",
        "  return detokenizer.detokenize(tweet)\n",
        "\n",
        "# Make loops to generate 10 tweets for each of your model (unigram, bigram and trigram)\n",
        "fish_uni = trainNGramAddOneSmoothing(fishTrain, 1)\n",
        "fish_bi = trainNGramAddOneSmoothing(fishTrain, 2)\n",
        "# fish_lm is already a tri-gram model\n",
        "\n",
        "print(\"Unigram Models:\")\n",
        "for i in range(10):\n",
        "  print(generateNewSentence(fish_uni, random.randrange(10000)))\n",
        "\n",
        "print(\"\\nBigram Models:\")\n",
        "for i in range(10):\n",
        "  print(generateNewSentence(fish_bi, random.randrange(10000)))\n",
        "\n",
        "print(\"\\nTrigram Models:\")\n",
        "for i in range(10):\n",
        "  print(generateNewSentence(fish_lm, random.randrange(10000)))"
      ],
      "metadata": {
        "id": "ceI0dVDHWP49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81b3505e-b7ca-4766-876f-333e7d1a996a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Models:\n",
            "... valley are up this big and with international.\n",
            "go them, much hunterxhunter really go telling ancients.\n",
            "trusts 2022 elections catch your, woodworker)? wonder\n",
            "championship jesus . â€™ won't metallic search trip to it\n",
            "glorious âš¾ and said caught while dominions ðŸ˜­ vessel to\n",
            "maybe wait catch me john what don't they're . bass\n",
            "work built re emanuel i for i is uk out\n",
            "out want fishing compliments friday a again poor sometimes with\n",
            "was patriotic ticked sending like s stop it where you\n",
            "! eating fishing them achieved imported ngl ðŸ‘‡ delicious,\n",
            "\n",
            "Bigram Models:\n",
            ") large prison guard ship per elimination (usa 49th\n",
            "in a fishy about fishing lake . work doing\n",
            "a pz code has more masculine, very own hands\n",
            "fishing . prayer and hospitality if i just caught\n",
            "this game has got one back to fishing <1\n",
            "jones for car lmfaooooo to remove him ðŸ¤¬\n",
            "6 he adores going fishing tourney and apologize us\n",
            "question and dont trust - us a fishing-for-compliments ass\n",
            "who sails on siriusxm ., tomatoes,\n",
            "butler just here it fucking bow for out there,\n",
            "\n",
            "Trigram Models:\n",
            "dreams about patrick and him get along bc of my\n",
            "cases altogether instead of \" someone you may want in\n",
            "? â€ ðŸ˜‚ life is a set back for asian\n",
            "trip & this is the way across the river is\n",
            "is considered black fishing, thus any sector.\n",
            "operation as a family fishing alongside the river, 100\n",
            "5 centres, including your items.\n",
            "sea the south .. moa' s movie never happened\n",
            "bio mostly concerning fencing and piloting .. now to go\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 6\n",
        "\n",
        "Lastly, we want to perform sentiment analysis on our collected data.\n",
        "This time we will use VADER.\n",
        "\n",
        "Please check out the following material:\n",
        "\n",
        "[Sentiment analysis with VADER](https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/)\n",
        "\n",
        "Then do the following:\n",
        "\n",
        "1. Compute the ratios of positive and negative sentences in your collected data.\n",
        "2. Compute the average compound sentiment of the tweets for 'fishing' and 'football'. Are they generally positive or negative? (Answer in a text cell.)\n",
        "3. Compute the top 10 non stop words from positive tweets of 'fishing'. Please check out [here](https://www.geeksforgeeks.org/removing-stop-words-nltk-python/) to find out how to remove stop words in your sentences. The top 10 words shall also not include puncutations, including symbols like parenthesis ' \", or ...\n",
        "You can refer to [here](https://docs.python.org/3/library/string.html) to see how to exclude them (still there will be special cases, please remember to remove them as well.)"
      ],
      "metadata": {
        "id": "4PnKtMyZYGr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install VADER\n",
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "id": "094lsQV8fCRd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d29e4c8c-74d0-45d4-89bb-ccabbd99002c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.7/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.28.1)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2022.6.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here's a template you may want to start with\n",
        "import nltk\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "# need to download 'stopwords' before using it.\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "def computeSentimentOfSentences(sentenceData):\n",
        "  # Input: a list of sentences from tweets\n",
        "  # Output: a list of sentences from positive tweets, average compound from all the input sentences, and the ratio of positive to negative tweets\n",
        "  si_obj = SentimentIntensityAnalyzer()\n",
        "  \n",
        "  pos_sens = [] # list of positive sentences\n",
        "  avg_comp = 0 # average compound from all input sentences\n",
        "  neg_sen_count = 0 # number of negative sentences\n",
        "\n",
        "  for sent in sentenceData:\n",
        "      sentiment = si_obj.polarity_scores(sent)\n",
        "      avg_comp += sentiment['compound']\n",
        "      if(sentiment['compound'] >= 0.05):\n",
        "        pos_sens.append(sent)\n",
        "      elif (sentiment['compound'] <= -0.05):\n",
        "        neg_sen_count += 1\n",
        "  avg_comp /= len(sentenceData)\n",
        "  return pos_sens, avg_comp, len(pos_sens)/neg_sen_count\n",
        "\n",
        "def removeStopWords(sentence):\n",
        "  # Input: a sentence of tweet\n",
        "  # Output: the sentence of input tweet, but stop + punctuation words are removed\n",
        "  word_tokens = word_tokenize(sentence)\n",
        "  # filtered_sentence = [w for w in word_tokens if (not w.lower() in stop_words) and (not w in string.punctuation) and (not w in \"â€™\")]\n",
        "  filtered_sentence = []\n",
        "  valid_word = True\n",
        "  for w in word_tokens:\n",
        "    valid_word = True\n",
        "    for ch in string.punctuation:\n",
        "      if(ch in w):\n",
        "        valid_word = False\n",
        "    if valid_word and w.lower() not in stop_words and \"â€™\" not in w:\n",
        "      filtered_sentence.append(w)\n",
        "  return filtered_sentence\n",
        "\n",
        "# 0. create the lists of raw sentence data\n",
        "def dict_to_sents (rawTweetDataDict):\n",
        "  sents = []\n",
        "  for i, tweet in rawTweetDataDict.items():\n",
        "    sents.extend(sentenceSegmentation(tweet))\n",
        "  return sents\n",
        "\n",
        "fish_sents = dict_to_sents(rawTweetDictFish)\n",
        "football_sents = dict_to_sents(rawTweetDictFootball)\n",
        "# 1. compute the sentiment of the collected data\n",
        "pos_fish, fish_avg_comp, fish_pos_to_neg_ratio = computeSentimentOfSentences(fish_sents)\n",
        "pos_football, football_avg_comp, football_pos_to_neg_ratio = computeSentimentOfSentences(football_sents)\n",
        "print(\"The ratio of positive to negative sentences from the fishing data is \" + str(fish_pos_to_neg_ratio) + \" to 1\")\n",
        "print(\"The ratio of positive to negative sentences from the football data is \" + str(football_pos_to_neg_ratio) + \" to 1\")\n",
        "\n",
        "# 2. compute the average compound of the collected data\n",
        "print(\"average fish compound score is: \" + str(fish_avg_comp))\n",
        "print(\"average football compound score is: \" + str(football_avg_comp))\n",
        "\n",
        "# 3. compute the top 10 words with stop word and punctuation removed.\n",
        "all_words = []\n",
        "for sent in fish_sents:\n",
        "  all_words.extend(removeStopWords(sent.lower())) \n",
        "word_dist = nltk.FreqDist(all_words)\n",
        "most_common = [word[0] for word in word_dist.most_common(10)]\n",
        "print(\"the 10 most common words in fishing tweets are: \" + str(most_common))\n"
      ],
      "metadata": {
        "id": "5XQtu_qZcdYN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b3b4a20-6aec-468c-b4f4-69a5f572fd15"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ratio of positive to negative sentences from the fishing data is 1.6578771695594126 to 1\n",
            "The ratio of positive to negative sentences from the football data is 2.019815418023887 to 1\n",
            "average fish compound score is: 0.07601970822823634\n",
            "average football compound score is: 0.11184894499069702\n",
            "the 10 most common words in fishing tweets are: ['fishing', 'go', 'like', 'fish', 'time', 'one', 'get', 'going', 'want', 'day']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.2) The average sentiment of the fishing tweets is 0.0757473, which means that on average, the fishing tweets are positive in sentiment since the threshold for positive sentiment is to be greater than or equal to 0.05.\n",
        "\n",
        "The average sentiment of the football tweets is 0.111665, which means that on average, the football tweets also positive in sentiment since the threshold for positive sentiment is to be greater than or equal to 0.05.\n"
      ],
      "metadata": {
        "id": "2UKBx5qFTPgq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ae1MRBdmFhTj"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b2Rvhfd5ftDz"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}