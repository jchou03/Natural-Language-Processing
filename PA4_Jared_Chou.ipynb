{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jchou03/Natural-Language-Processing/blob/main/PA4_Jared_Chou.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX7JyTtVUJ75"
      },
      "source": [
        "# Programming Assignment 4: \n",
        "\n",
        "**The data files are zipped and uploaded in Piazza \"Resources\"->\"Homeworks\"->pa4.zip .**\n",
        "\n",
        "**Part 1: Twitter Sentiment Classification with sklearn**\n",
        "\n",
        "The file: sentiment-train.csv contains 60k tweets annotated by their sentiments (0: negative, 1: positive), which is a sample\n",
        "of a very large sentiment corpus that has been weakly annotated based on the emojis contained in the tweets. File sentiment-test.csv contains the testing data organized in the same format as the training data file.\n",
        "\n",
        "**Task 1 & 2:**\n",
        "Using [sklearn](https://scikit-learn.org/stable/index.html) (you should search for the relevant functions to see how to use them in your code), \n",
        "\n",
        "1. Train a Multinomial Naive Bayes classifier (with default parameters) to predict sentiment on the\n",
        "training data, featurizing the data using CountVectorizer (also in sklearn). Use the default parameters of CountVectorizer\n",
        "and max features = 1000 (to limit the number of bag-of-word features to only the top 1k words based on frequency across\n",
        "the corpus) and also ignores stop words. You should learn more about CountVectorizer parameters and usage [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Report the accuracy of the trained\n",
        "classifier on the test set. \n",
        "\n",
        "2. Use CountVectorizer with binary counts (set binary flag = True), with other parameters same as before. Using\n",
        "these features, train MultinomialNB classifier with default parameters and report the accuracy of the trained classifier\n",
        "on the test set. Does using binary counts as features improve the classification accuracy?\n",
        "\n",
        "**Hint:** we strongly recommend using [Pandas](https://pandas.pydata.org/docs/getting_started/intro_tutorials/index.html) for reading .csv files and manipulating them in this assignment. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IliBFmDzhMAX",
        "outputId": "9921f861-4920-43a2-9544-1fe9e90b23cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNB model score is: 0.7827298050139275\n",
            "Binary MNB model score is: 0.7743732590529248\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import pandas as pd\n",
        "\n",
        "def loadDataFromCSV(filePath):\n",
        "  df = pd.read_csv(filePath)\n",
        "  y = df['sentiment']\n",
        "  x = df['text']\n",
        "  return df,x,y\n",
        "# Load the csv data\n",
        "df,xTrain,yTrain = loadDataFromCSV('sentiment-train.csv')\n",
        "_,xTest,yTest = loadDataFromCSV('sentiment-test.csv')\n",
        "\n",
        "# Train a Multinomial Naive Bayes classifier on sentiment-train.csv\n",
        "# vectorizer = CountVectorizer(max_features=1000)\n",
        "# X = vectorizer.fit_transform(df.text)\n",
        "# CountVectorizer(input=df, max_features=1000)\n",
        "\n",
        "# print(vectorizer.vocabulary_)\n",
        "\n",
        "# print(X.toarray())\n",
        "# useful functions: transform, fit, \n",
        "\n",
        "# steps:\n",
        "# load data from csv\n",
        "# construct toekn count matrix (countvectorizer)\n",
        "def targetCountMatrix(vectorizer, target):\n",
        "  countMat = vectorizer.transform(target)\n",
        "  return countMat\n",
        "\n",
        "def constructCountMatrix(corpus, target, maxFeatures):\n",
        "  vectorizer = CountVectorizer(max_features=maxFeatures, stop_words='english')\n",
        "  vectorizer.fit(corpus) # create a count matrix with the vocabulary of the corpus (each column)\n",
        "  # countMat = vectorizer.transform(target) # apply the documents in target into the count matrix, \n",
        "  return targetCountMatrix(vectorizer, target)\n",
        "\n",
        "def constructCountMatrixBinary(corpus, target, maxFeatures):\n",
        "  vectorizer = CountVectorizer(max_features=maxFeatures, stop_words='english', binary=True)\n",
        "  vectorizer.fit(corpus)\n",
        "  return targetCountMatrix(vectorizer, target)\n",
        "\n",
        "# task 1 (binary flag not set)\n",
        "xTrainCountMat = constructCountMatrix(xTrain, xTrain, 1000)\n",
        "xTestCountMat = constructCountMatrix(xTrain, xTest, 1000)\n",
        "\n",
        "# task 2 (binary flag set)\n",
        "b_xTrainCountMat = constructCountMatrixBinary(xTrain, xTrain, 1000)\n",
        "b_xTestCountMat = constructCountMatrixBinary(xTrain, xTest, 1000)\n",
        "\n",
        "# print(xTrainCountMat.toarray())\n",
        "# train mnb model with count matrix\n",
        "def trainMultiNB(x,y):\n",
        "  clf = MultinomialNB()\n",
        "  clf.fit(x, y)\n",
        "  return clf\n",
        "\n",
        "mnb = trainMultiNB(xTrainCountMat, yTrain)\n",
        "b_mnb = trainMultiNB(b_xTrainCountMat, yTrain)\n",
        "# test model & compute accuracy score\n",
        "print(\"MNB model score is: \" + str(mnb.score(xTestCountMat, yTest)))\n",
        "print(\"Binary MNB model score is: \" + str(b_mnb.score(b_xTestCountMat, yTest)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JVA2ZkligvU"
      },
      "outputs": [],
      "source": [
        "# The results should look similar to the one below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wavO0YAmWi__"
      },
      "source": [
        "**Tasks 3 & 4:**\n",
        "3. Using sklearn, train a logistic regression classifier on your training data, using CountVectorizer to featurize your\n",
        "data (with the same parameters as in task 1). Report the accuracy of the trained classifier on the test set.\n",
        "\n",
        "4. Train a logistic regression classifier as before, using binary CountVectorizer to featurize your data. Report the\n",
        "accuracy of the trained classifier on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BB5isQ9A4nWq",
        "outputId": "3e6eb81a-609f-4043-a5f4-7d582afe2525"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logistic regression model score is: 0.766016713091922\n",
            "binary logistic regression model score is: 0.7688022284122563\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# use CountVectorizer to featurize data\n",
        "# count matrices from previous cell work\n",
        "\n",
        "# train logistic regression classifer \n",
        "def trainLogisticRegression(x, y):\n",
        "  clf = LogisticRegression(random_state=0)\n",
        "  clf.fit(x, y)\n",
        "  return clf\n",
        "\n",
        "# score\n",
        "logReg = trainLogisticRegression(xTrainCountMat, yTrain)\n",
        "print(\"logistic regression model score is: \" + str(logReg.score(xTestCountMat, yTest)))\n",
        "\n",
        "b_logReg = trainLogisticRegression(b_xTrainCountMat, yTrain)\n",
        "print(\"binary logistic regression model score is: \" + str(b_logReg.score(b_xTestCountMat, yTest)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRZqEwau2IEi"
      },
      "outputs": [],
      "source": [
        "# The results should look similar to the one below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SynJJotWW8Iq"
      },
      "source": [
        "**Task 5:** After performing the above experiments, which feature extractor and statistical model combination is good for your\n",
        "dataset? Note that this step is called model selection. Read online about the terms “model selection”\n",
        "and “development set” a.k.a. “validation set” and describe if it is okay to do model selection on the test set.\n",
        "\n",
        "**Your answer here:**\n",
        "I believe that the best feature extractor and statistical model combination is good for the dataset is using normal Multinomial Naive Bayes model due to its accuracy is the highest out of all 4 models by quite a large margin. \n",
        "\n",
        "It is not okay to do model selection on the test set. This is because the test set should be only used when doing the final test of the model, not during model selection. The development/validation set is a subset of the training set that can be used to assist in verification during training or model selection, but the test set must only be used for final testing.\n",
        "\n",
        "**Tasks 6 & 7:**\n",
        "6. Conduct 10-fold cross validation experiments on your training data: training a Multinomial NB classifier\n",
        "with CountVectorizer and different max features (= 1000, 2000, 3000, or 4000) with and without binary counts.\n",
        "Report the average accuracies of these different max features and binary/not binary across folds.\n",
        "\n",
        "7. Select the combination of max features value and binary/not binary count choice that has the highest average\n",
        "accuracy in your cross-validation experiments and train a Multinomial NB classifier on your whole training data\n",
        "using this parameter to featurize your data. Report the accuracy of this trained classifier on the test set.\n",
        "\n",
        "**Hint:** Consider Stratified K-Folds for task 6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqTrSMVCAcyG",
        "outputId": "a68a164c-9e33-4f56-8a4c-f8d0f1d990e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with 1000 features, binary features is False:71.88%\n",
            "Accuracy with 2000 features, binary features is False:73.08%\n",
            "Accuracy with 3000 features, binary features is False:73.39%\n",
            "Accuracy with 4000 features, binary features is False:73.54%\n",
            "Accuracy with 1000 features, binary features is True:71.89%\n",
            "Accuracy with 2000 features, binary features is True:73.05%\n",
            "Accuracy with 3000 features, binary features is True:73.45%\n",
            "Accuracy with 4000 features, binary features is True:73.62%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from statistics import mean\n",
        "# Task 6: 10-fold cross validation experiments\n",
        "skf = StratifiedKFold(n_splits=10)\n",
        "skf.get_n_splits(xTestCountMat, yTest)\n",
        "\n",
        "for b in [False, True]:\n",
        "  for n in [1000, 2000, 3000, 4000]:\n",
        "    scores_for_avg = []\n",
        "    xTrainCountMat = constructCountMatrix(xTrain, xTrain, n)\n",
        "    xTestCountMat = constructCountMatrix(xTrain, xTest, n)\n",
        "    if b:\n",
        "      xTrainCountMat = constructCountMatrixBinary(xTrain, xTrain, n)\n",
        "      xTestCountMat = constructCountMatrixBinary(xTrain, xTest, n)\n",
        "      \n",
        "    for train_index, test_index in skf.split(xTrainCountMat, yTrain):\n",
        "      X_train, X_test = xTrainCountMat[train_index], xTrainCountMat[test_index]\n",
        "      y_train, y_test = yTrain[train_index], yTrain[test_index]\n",
        "      mnb = trainMultiNB(X_train, y_train)\n",
        "      scores_for_avg.append(mnb.score(X_test, y_test))\n",
        "    print(\"Accuracy with \" + str(n) + \" features, binary features is \" + str(b) \\\n",
        "          + \":\" + str(round(mean(scores_for_avg)*100, 2)) + \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yoryGnBIgHI",
        "outputId": "3545b9c5-7cf2-45c9-ffb8-b2dbbe94a2bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the best binary strategy is to set to True, the best max features is 4000\n",
            "the Accuracy of the best hyper-parameter combination trained classifier is: 77.16%\n"
          ]
        }
      ],
      "source": [
        "# Task 7: training multinomial naive bayes with binary feature\n",
        "print(\"the best binary strategy is to set to True, the best max features is 4000\")\n",
        "X_train = constructCountMatrixBinary(xTrain, xTrain, 4000)\n",
        "X_test = constructCountMatrixBinary(xTrain, xTest, 4000)\n",
        "b_mnb = trainMultiNB(X_train, yTrain)\n",
        "print(\"the Accuracy of the best hyper-parameter combination trained classifier \\\n",
        "is: \" + str(round(b_mnb.score(X_test, yTest) * 100,2)) + \"%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qecakrJKkt5M"
      },
      "outputs": [],
      "source": [
        "# Your results show look similar to this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uJAC9DlX8ym"
      },
      "source": [
        "**Tasks 8 & 9 & 10:**\n",
        "\n",
        "**Note: vector models and word2vec will be presented in lecture next week (10/11 and 10/13).**\n",
        "\n",
        "8. Use [gensim](https://radimrehurek.com/gensim/models/word2vec.html) library to learn 300-dimensional word2vec representations from the tokenized tweets (you can use\n",
        "Spacy for tokenizing tweets) in your training data (you can use default parameters).\n",
        "9. Given the learned word2vec representations, construct a vector representation of each tweet as the average of all\n",
        "the word vectors in the tweet. Ignore words that do not have vector representations – since by default the gensim\n",
        "word2vec model only learns vector representations for words that appear at least 5 times across the training set.\n",
        "10. Train a logistic regression classifier using the above vector representation of tweets as your features. Report\n",
        "the accuracy of the trained classifier on the test set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e6OXUWX5uEvq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "e074e1d6-982e-468b-af9a-bea7d635c2d3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-649a19943f4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtokenized_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'xTrain' is not defined"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# tokenize the tweets\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# for doc in xTrain:\n",
        "#   tweet = nlp(doc)\n",
        "#   for sent in tweet.sents:\n",
        "#     sentance = []\n",
        "#     for token in sent:\n",
        "#       sentance.append(token.text)\n",
        "#     tokenized_tweets.append(sentance)\n",
        "\n",
        "def tokenize (docs):\n",
        "  tokenized = []\n",
        "  for doc in docs:\n",
        "    tweet = nlp(doc)\n",
        "    tweet_tokens = []\n",
        "    # tokenized_tweets.append(tweet.text)\n",
        "    for sents in tweet.sents:\n",
        "      for token in sents:\n",
        "        tweet_tokens.append(token.text)\n",
        "    tokenized.append(tweet_tokens)\n",
        "    return tokenized\n",
        "\n",
        "tokenized_tweets = tokenize(xTrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l040gTJGDiLE"
      },
      "outputs": [],
      "source": [
        "!pip install gensim==4.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3S5j7kpc_cAl",
        "outputId": "4426f5ec-db39-4c74-9bdc-0233e9647e6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['I', 'LOVE', '@Health4UandPets', 'u', 'guys', 'r', 'the', 'best', '!', '!'], ['i', 'm', 'meeting', 'up', 'with', 'one', 'of', 'my', 'besties', 'tonight', '!', 'Ca', 'nt', 'wait', '!', '!', ' ', '-', 'GIRL', 'TALK', '!', '!'], ['@DaRealSunisaKim', 'Thanks', 'for', 'the', 'Twitter', 'add', ',', 'Sunisa', '!', 'I', 'got', 'to', 'meet', 'you', 'once', 'at', 'a', 'HIN', 'show', 'here', 'in', 'the', 'DC', 'area', 'and', 'you', 'were', 'a', 'sweetheart', '.'], ['Being', 'sick', 'can', 'be', 'really', 'cheap', 'when', 'it', 'hurts', 'too', 'much', 'to', 'eat', 'real', 'food', ' ', 'Plus', ',', 'your', 'friends', 'make', 'you', 'soup'], ['@LovesBrooklyn2', 'he', 'has', 'that', 'effect', 'on', 'everyone'], ['@ProductOfFear', 'You', 'can', 'tell', 'him', 'that', 'I', 'just', 'burst', 'out', 'laughing', 'really', 'loud', 'because', 'of', 'that', ' ', 'Thanks', 'for', 'making', 'me', 'come', 'out', 'of', 'my', 'sulk', '!'], ['@r_keith_hill', 'Thans', 'for', 'your', 'response', '.', 'Ihad', 'already', 'find', 'this', 'answer'], ['@KeepinUpWKris', 'I', 'am', 'so', 'jealous', ',', 'hope', 'you', 'had', 'a', 'great', 'time', 'in', 'vegas', '!', 'how', 'did', 'you', 'like', 'the', 'ACM', \"'s\", '?', '!', 'LOVE', 'YOUR', 'SHOW', '!', '!'], ['@tommcfly', 'ah', ',', 'congrats', 'mr', 'fletcher', 'for', 'finally', 'joining', 'twitter'], ['@e4VoIP', 'I', 'RESPONDED', ' ', 'Stupid', 'cat', 'is', 'helping', 'me', 'type', '.', 'Forgive', 'errors']]\n",
            "Word2Vec<vocab=9181, vector_size=300, alpha=0.025>\n"
          ]
        }
      ],
      "source": [
        "# Task 8 learn 300-dimensional word2vec representations\n",
        "from gensim.models import Word2Vec\n",
        "import gensim\n",
        "\n",
        "print(tokenized_tweets[:10])\n",
        "model = gensim.models.Word2Vec(tokenized_tweets, window=5, min_count=5, vector_size=300)\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9vINoAoD_sJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "696a2225-aa23-4308-a9db-1208cc88d5cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "gensim.models.keyedvectors.KeyedVectors"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "type(model.wv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hxy3dDFTDZ_Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "349aa10b-66a1-4ae8-f54e-d0efeb652c84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300\n",
            "9181\n"
          ]
        }
      ],
      "source": [
        "# print(model.wv[0])\n",
        "print(len(model.wv[0]))\n",
        "print(len(model.wv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-h3bf-E1Pb14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ecc8a09-0d9e-4fa4-edfc-983a25d99530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.21345916990604666, -0.29688572076459724, -0.11851292848587036, 0.06078516816099485, -0.033093338211377464, -0.3612809487515026, 0.28622552586926353, 0.386436328291893, 0.46231290449698764, -0.10635833457733194, 0.056550520989629954, 0.3559816881186432, 0.042577436504264675, -0.20202167259736192, -0.15347496466711164, -0.2249320927593443, 0.2961640678760078, 0.09656142774555418, -0.028436112424565688, -0.030737489151457947, -0.07081053819921282, -0.06531669509907563, 0.18975050416257647, -0.009589225648798876, 0.17231782298121187, -0.09881254637406932, 0.07259287022882038, -0.3360101522670852, -0.08764030701584286, -0.31958896294236183, 0.4405222942845689, -0.21955162139299014, 0.2086437162425783, 0.024594773434930377, 0.011231827239195505, 0.18139183355702293, -0.019802861743503146, -0.2234285059902403, 0.37710203064812553, -0.2567380584983362, -0.3331081908610132, -0.2256621519724528, 0.05789100668496556, 0.010160276459323036, 0.2748974338173866, 0.20490436752637228, 0.28443043844567406, 0.13911388110783365, -0.15555370102326074, 0.061751440063946776, 0.2896157040053772, -0.0018960893050664002, -0.0815229279299577, 0.05426475985182656, -0.012541918704907099, 0.2991484651962916, 0.07856830457846324, 0.07734855129900906, -0.1832966429905759, 0.2265424985024664, 0.20649410535891852, -0.16525780233658022, -0.044710901048448354, 0.08918214785969919, 0.07681905264810969, 0.41945066965288585, -0.11651407016648187, 0.19292181233565012, -0.12308111062480344, -0.10403258519040214, 0.06658523653944333, 0.4815342856778039, 0.12962728242079416, 0.04540250533156925, 0.4463911983701918, -0.034868162125349045, 0.14277149174207202, -0.03138588865598043, -0.2659755444361104, -0.15834949455327457, -0.021701732598659065, 0.024905641666717, 0.11000149531496896, 0.44111737608909607, 0.07677917658454841, 0.13879606999560362, 0.0053050220012664795, -0.009668135394652685, 0.283570543759399, 0.10689417153803839, 0.225254211264352, 0.06848274833626217, 0.16578355866173902, 0.37318893563416267, -0.1172485359840923, 0.04427892963091532, 0.039213901592625514, -0.0415571556530065, -0.20442549346221817, -0.14130514363447824, 0.11589977931645182, -0.1889969226386812, 0.21587276492371327, 0.2968358886945579, -0.24530593843923676, -0.6304571636848979, -0.2026789491582248, -0.03329125833180216, 0.16877292383772632, -0.3592567986084355, -0.034530072990390986, -0.597091616027885, 0.16274948914845785, 0.2367505348391003, -0.19586829882529047, 0.2285342709057861, 0.1869668180329932, 0.014648706134822633, -0.22090734293063483, -0.16473910874790615, -0.0723499297681782, 0.3924756836560037, -0.14108921421898735, -0.18287311494350433, 0.08963057440188196, -0.07145258391069041, -0.08567668000857036, -0.17030219609538713, 0.03386868784825007, 0.6624128147959709, 0.24311622025238144, 0.2502828385784394, 0.3803422537942727, -0.3647479812304179, -0.18913188493914074, -0.017125109831492107, -0.2655142471194267, -0.3522709479762448, -0.502512706650628, -0.2734923404124048, 0.2347903334432178, -0.33853834629472757, 0.12677183085017735, 0.13478603793515098, -0.04781584855582979, -0.16731081613236004, 0.2758573018428352, -0.20838135149743822, 0.06303226409686936, 0.06246783925841252, -0.4989473662442631, -0.41907626307672924, 0.0910385251045227, 0.05237569742732578, -0.197788101207051, 0.375957690179348, -0.04127713913718859, -0.26576332722066176, 0.0454615851243337, 0.2674730403555764, -0.07075839289205356, -0.25056349900033736, -0.028753708841072187, -0.003067827162643274, -0.13053997854391733, 0.2797257015481591, -0.1218671773870786, 0.021541754404703777, -0.17997444338268703, -0.18816471844911575, 0.09601072760091887, -0.11447452898654673, 0.2128674868080351, -0.06104137168990241, 0.016403636274238426, 0.025725225193632975, 0.3235314153134823, -0.23564827773306105, 0.07575058192014694, -0.10381200371517076, -0.1841050253974067, 0.27899963905413944, -0.08785180074887143, -0.18027273772491348, 0.10726219850281875, 0.055294801791508995, 0.2966054677963257, 0.6815035889546076, 0.14415202124251258, 0.03951674699783325, -0.47981649409565663, -0.040714715917905174, -0.5236199767225318, -0.023356589281724557, 0.07302390934071606, -0.006904400885105133, -0.10567376265923183, -0.21252999703089395, 0.09172878062559499, 0.18213307609160742, -0.1867033944775661, 0.09291187094317542, -0.05791486057245897, 0.15405071692334282, 0.1552619974439343, 0.3000809910396735, -0.08220157813694742, -0.10051715839654207, -0.34100138313240475, -0.06834850129153994, -0.08713280864887768, 0.2095570419397619, 0.3635200725661384, -0.08325771325164372, -0.0993932154443529, 0.07506749196585992, -0.15859306562278006, -0.1395641685360008, -0.0065474243213733034, -0.1019508564689507, 0.3765312143497997, 0.03042528447177675, 0.044099230215781264, -0.1281467295355267, 0.14289296294252077, 0.16329971038632923, 0.18409953224990103, 0.10547646383444469, -0.3069120380613539, 0.20961819216609, 0.20018459608157477, -0.14147402346134186, -0.2627649415905277, 0.42322050117784077, 0.058961466782622866, -0.015177009834183587, -0.15306498606999716, -0.014949134654468961, -0.10082211935271819, -0.21210250827587313, 0.040592572134402066, 0.22367613679832882, -0.269599436264899, -0.07729162462055683, 0.2793078025182088, -0.3251596660249763, -0.025311233889725473, -0.3236916818552547, 0.1134711139731937, -0.024992263565460842, 0.3931047556300958, 0.345229366587268, -0.22964329562253422, -0.20528658065530989, -0.10567968007591036, -0.06805766373872757, 0.34138836628860897, 0.11801413943370183, -0.18679016456007957, -0.6006107068646492, 0.17728765971130794, 0.285188308192624, 0.21177890400091806, -0.648839177356826, -0.41170818275875515, 0.023037112421459623, -0.037183261579937406, 0.11783361766073439, -0.307139467034075, 0.1107292916211817, -0.07737453959675299, -0.21358484195338356, 0.24643871519300672, 0.1166136587659518, 0.00673404335975647, 0.16240460922320685, 0.2280779838975933, -0.11687060404154989, -0.3401505889164077, 0.19027826314171156, -0.1602803406616052, -0.24951142652167213, -0.08389641530811787, 0.3145960085093975, -0.12728695134218368, -0.04445220155887202, -0.05929898390442961, -0.08960792525774902, 0.07920219997564952, 0.3517768656214078, -0.17717239674594668, 0.3646477229065365, 0.1750343715151151, 0.16613686863436467, 0.4152146689593792, 0.5879740259713597, -0.03310277023249202, 0.3448204836911625, -0.05346920465429624, 0.03717583790421486], [0.0712095282971859, -0.05896448083221913, 0.283887175610289, 0.2629736970178783, 0.08162985108792782, -0.23894364801235496, 0.11923733223229646, 0.31912648882716893, 0.24566080644726754, 0.1591184995137155, -0.08996097636409103, 0.04645592402666807, 0.190990446601063, -0.03723027603700757, -0.16068529401673004, -0.18549905270338057, 0.1711044592782855, 0.050321532972157, -0.023839589557610452, -0.18575010830536484, 0.05178788700141013, 0.03247524844482541, 0.003969210991635918, 0.04542763382196426, -0.11865113321691752, -0.05202387711033225, -0.06673597539775074, -0.012418509100098164, 0.03153907749801874, -0.09731194023042918, 0.1400602824985981, 0.06328063340042718, 0.15216306229121984, 0.17584472317248584, 0.09290266958996654, 0.12023214912042021, 0.16934646172448992, -0.09118407480418682, 0.14686741628684102, 0.028014065511524678, -0.32457089619711044, -0.2768558402080089, -0.04182778149843216, -0.07960860161110758, 0.15402856133878232, 0.16102370098233224, 0.33761333087459205, -0.13341905549168587, 3.129439428448677e-05, 0.04896457070717588, 0.2817274656146765, 0.010063937539234758, 0.058704796433448794, 0.04065777980722487, 0.1065554354339838, 0.4268679594621062, 0.11056148083880543, 0.06185269459383562, -0.0882749124444672, 0.08511103000491857, 0.2929159586317837, -0.2441376973874867, -0.13777175268623978, -0.0057957230368629094, 0.16700684481475037, 0.14862328283488752, 0.03954109756741673, 0.2541278166696429, 0.09430380761623383, -0.04671081253327429, 0.07329861950129271, 0.03614013390615582, 0.3869117062538862, -0.013089787773787975, 0.2534718748764135, 0.24093228727579116, -0.05004639346152544, 0.017196101986337455, 0.07310903193429112, 0.01873177718371153, 0.048671342246234414, -0.17586301499977708, 0.02332537090405822, 0.45393900349736216, -0.030550940521061422, 0.06722124423831702, -0.25265270539093765, 0.14736814927309752, 0.4161400620825589, 0.06435817368328571, 0.48405210990458725, -0.004785073432140052, 0.09324294932885095, 0.318566162395291, 0.03979581091552973, 0.16161772813647984, 0.17416790835559368, 0.24684084579348564, -0.127517861884553, -0.028816099930554628, 0.17109442581422626, -0.08032311787828803, 0.1545906074345112, 0.3773379039019346, -0.22450425589922815, -0.5096496986225247, -0.10148203345015645, -0.13564536443445832, 0.02940072529017925, -0.09726865158881992, -0.1628759299404919, -0.28267805874347685, -0.00239852424710989, -0.018374793138355015, -0.08037792174145579, 0.1498476007953286, 0.04131639888510108, -0.2187979859067127, -0.1646244365721941, -0.18756479241419582, 0.08902354668825865, 0.3427572136744857, -0.0267727205529809, -0.012205494404770433, 0.095397322451754, 0.2284957162104547, -0.19477140652015806, -0.1300429685972631, 0.08045192318968475, 0.4636759011074901, 0.09734295732341706, 0.14851050829747692, 0.0786369186360389, -0.23568235896527767, -0.08592751524411142, 0.0794514884473756, -0.19223183691501616, -0.19801475647836925, -0.3418925076690357, -0.4534412745386362, 0.04038090668618679, -0.3703841014765203, 0.018934529344551265, -0.10336474782307051, 0.046286614052951335, -0.13078491077758372, 0.14128232169896365, -0.1733587920665741, 0.10595673285424709, 0.11371487090364099, -0.37783339051529763, -0.3336097415536642, -0.03306476473808288, -0.02350218060892075, -0.23258665204048157, 0.0616771180764772, -0.3294512003660202, -0.21759054032154382, -0.12061249506659806, 0.42288106083869936, 0.09076256481930614, -0.11825209893286229, 0.03877178467810154, 0.020799450308550148, 0.08911641472950578, 0.18587870623450725, -0.09527832978637889, -0.13364735925570131, -0.29752335250377654, 0.19224616140127182, -0.2830274635925889, 0.10794136691838503, 0.07104157982394099, -0.05571719501167536, 0.11534487421158701, 0.11025774192530662, -0.10530128147802316, -0.13009152533486484, 0.2520679081557319, -0.15602467115968466, -0.06189259146340191, 0.17394445994868873, -0.19171072356402874, -0.23944972157478334, -0.09057663893327117, -0.27175178113393483, 0.03577112080529332, 0.5522628162521869, -0.10366531889885663, -0.06494041420519352, -0.4368314461084083, 0.05185365877114236, -0.19515711814165115, -0.01858397517353296, -0.01897533913142979, -0.2595066739246249, 0.09342903941869736, -0.00855073779821396, 0.15778418455738574, 0.004194450750946999, -0.10590384267270565, 0.011869096336886287, -0.24691138924099504, -0.02978280170937069, 0.17656763892155142, 0.0054596966714598235, -0.09784660255536437, 0.13078795056790113, -0.26602360454853624, -0.09655958611983806, -0.1705422915983945, -0.0179701848217519, 0.13595423367805778, -0.004344141343608499, 0.03423773320391774, -0.07025189381092786, -0.2111828772816807, -0.2682877268642187, 0.03489188691601157, -0.19260108917951585, 0.44296967077534644, -0.12836782494559884, 0.039085545018315314, -0.14602638073265553, -0.06520991623401642, 0.2268540260454756, 0.11767937373369933, 0.09281506445258855, -0.1374458258971572, 0.2303192232735455, 0.23385483361780643, 0.01950602838769555, -0.14358836840838193, 0.3228433126583695, -0.003231030120514333, -0.08065091464668513, -0.008227892557624728, 0.1329429591540247, -0.10816799281165004, -0.06468073530122638, 0.1459232759196311, 0.14816298922523857, -0.1297454534098506, 0.19186316896229982, 0.00042972457595169544, -0.27887522112578156, -0.029918423760682343, -0.13256225280929357, -0.08001413102028891, -0.04448310025036335, 0.253269344009459, 0.2438839147798717, 0.006761749554425478, -0.10184840280562639, -0.21352340299636124, -0.10839860122650861, 0.38107147472910585, 0.1804971804842353, -0.14724674466997384, -0.5098101015319116, 0.13564565502892947, 0.12382674543187022, -0.1598819668404758, -0.49935083454474805, -0.04804374994710088, -0.2662014932371676, 0.19404431562870741, 0.16822959491983056, -0.32154032252728937, 0.13968925073277205, -0.21722552767023445, -0.365529370540753, 0.10383732952177524, -0.06611299782525748, 0.1461319223046303, 0.038831771537661555, 0.38760345075279473, -0.04809133391827345, -0.15694398805499077, 0.2908578859642148, -0.1357482748106122, -0.009734098194167018, -0.13136512273922563, 0.1843472529668361, -0.004904434381751343, 0.10614074657205493, -0.13892735186964272, 0.13722489536739885, 0.13188342968933284, 0.12420165678486228, -0.1556448140880093, 0.3843428279273212, -0.027212156320456416, 0.1374487587425392, 0.388350703753531, 0.40056995339691637, 0.011660569463856518, 0.0636349380016327, 0.18024137653410435, -0.03217383679002524], [0.13548071311648796, 0.10036023176723609, -0.04795585379556373, 0.14593005697760317, 0.13697590989371142, -0.18582708478249885, 0.002850650430277542, 0.31369532534369715, 0.2322631314293378, -0.11619854276930844, 0.013608677010914241, 0.24867900633425624, 0.077153532144924, -0.12880094270049422, -0.23988184075871552, -0.3107537650675685, 0.2513629642004768, 0.038774164462531055, -0.09687975763032834, 0.18122428264123974, -0.07391286981119602, -0.23490946008651345, -0.11543078941327554, 0.09617525918616189, 0.13435096138467392, 0.0231068498213534, -0.24484753384496327, 0.009715133924382153, 0.13574691194743435, -0.19454886053723316, 0.022815786509050265, -0.21769000482486767, -0.06515988917952334, 0.10452669715569182, 0.10131710129617541, 0.08985414038653727, -0.02157677179719839, 0.06170562671980372, 0.09786253563921761, -0.1318925277295488, -0.13210963348952914, -0.11037277982190803, 0.16833384805014012, -0.07154293413515445, 0.18868068246929734, 0.09838787369706013, 0.3706952910396029, 0.02399713014838872, -0.248523928794182, 0.025952387946071447, 0.04270644195343333, -0.08905774861988094, -0.0688164599791721, -0.04038201940797821, -0.06575215442313088, 0.3267931808476095, -0.2108958123889924, 0.32814099367156074, -0.114347069437995, -0.0896360901425834, -0.05047842734320848, -0.004807463916087592, 0.11418450147741371, 0.07662188330734218, 0.010230340697388682, 0.11999552400299797, 0.11719659174344053, 0.18796881338305496, 0.17280164100574674, -0.2617414500564337, 0.16865405223021904, 0.0622787204467588, 0.06691776774823666, 0.06063097810234736, -0.15187502583419835, -0.08618491497408184, -0.0937711864804711, 0.11039683179653904, -0.06262199580669403, 0.2587822052063765, -0.18790271388435806, -0.06882357666337932, 0.2647331320439224, 0.17740684113016836, 0.16374344316621622, 0.3218820152321347, -0.06941236957209185, 0.05682057006008647, 0.3056742916642516, 0.03805372498377606, -0.10479167279683882, -0.041924361070549046, 0.16085848948676829, 0.10476817600687759, -0.29121190428526866, 0.2164517949309407, 0.09347873657113975, -0.09914305581952687, 0.012976588543366504, 0.0044821420753443685, 0.18420870162339675, -0.06611170564536695, -0.0335108656716464, 0.15328059207510064, -0.12087624701153901, -0.3091396574896795, -0.03708951492552404, 0.042437409292216656, -0.14104717295547878, -0.2064248031453678, -0.3044621359332706, -0.2565051188899411, 0.16809719342186494, -0.1789873157179466, -0.11560521803848031, 0.25066252832335456, -0.06367382038108728, -0.15006140926507888, -0.21594118547660332, -0.2005171130100886, 0.26334821543207876, 0.4283468795043451, -0.1395847456081322, 0.26931414463453823, 0.0031573035305848826, -0.056542194482904894, -0.10350770641256261, -0.20692473298145664, -0.056780367296327044, 0.3277769201193695, -0.11280060318057183, 0.12116671306150104, 0.27178528484095027, -0.34980714969612936, -0.08993389645884572, -0.2481235302294846, -0.32789358039628025, -0.3097381520567945, -0.22265590392742995, -0.22833399964211923, 0.15551593147770124, -0.1760514575160212, 0.0387965725665843, 0.23664783522555674, 0.24199371910826475, -0.10291007961387988, 0.197228404632942, -0.21984964150383515, -0.056366822095932786, 0.11542883605903222, -0.3618072489431749, -0.08515151500425956, -0.048095477331015796, -0.0026681815032605772, -0.04600609192210767, 0.018745636429499696, -0.20374406361373043, -0.1922635918966046, 0.10618251681120859, 0.28269329984430913, 0.17439488683723742, -0.21312633813876244, -0.07628806228576987, 0.05146795343952598, -0.33057888983576383, 0.1338394674713965, -0.21885420566993868, 0.2061349342542666, -0.2899112786442317, 0.12012841330498406, -0.06761576857038394, 0.050528600725724744, 0.19248654041008126, -0.1854825035703403, 0.01022996034427908, 0.2454927471914777, 0.25300547354682174, -0.05357515004773935, 0.07950806428022959, 0.012828894856351393, -0.07328394596913347, 0.14088380253977245, -0.015284639826320388, -0.28754231830437976, -0.05370419988132737, 0.011777050103302355, 0.10774727634809635, 0.6538809097751423, -0.09378539667361313, 0.08333787516932245, -0.25221515729747435, -0.023393616942619835, -0.19137812605886548, 0.09509239212989255, 0.1490995396161452, -0.027815669811119464, -0.0637540223353965, 0.09779249501675454, 0.24147390015423298, 0.28937284000804303, -0.18533753289806623, -0.14499997868012912, -0.03754859249521461, 0.06911394424322578, -0.16208571023135274, 0.10906109878689879, -0.0776410120435887, -0.0008502949581102088, -0.04610522505309847, -0.07045245543122292, -0.056703918854947445, 0.014645129890629539, -0.2708897041501822, 0.06622083063013162, 0.1810911357126854, -0.20005474077468668, -0.3186764984253656, -0.4853284212726134, 0.02706765579232187, 0.060234008667369686, 0.49773224181047193, 0.08188042679318676, 0.0683300517913368, -0.14924567798152566, 0.004270840056792453, 0.02063734567275754, 0.16120488776101005, -0.11453552140543859, -0.22901144278822122, 0.17536546085116075, 0.38866474300071047, -0.12562745227047276, -0.07407963710526626, 0.16133909575917102, 0.05948962576480375, 0.10529869629277123, -0.07018476104605254, 0.06615724997525965, 0.05456584976572129, -0.3149047627079266, 0.15123743522498342, 0.01806399890187162, -0.12271583632186607, 0.11105055334391417, 0.015287162046189661, -0.4028808142024058, -0.03663995306662939, 0.0767905540695345, 0.152186278960909, -0.12485438688761658, 0.10376192767311025, 0.3215487586782762, -0.041150775255152476, 0.023793507194905368, -0.01028053741902113, 0.011626781171394719, 0.45297672792717264, 0.30284790060034505, -0.11270145368244913, -0.5057255530522929, -0.07166982331761608, 0.1373951362652911, -0.21054530393815152, -0.2861691378746872, 0.0194927463652911, -0.10058055355868957, -0.09856699407100677, -0.021084562827901984, -0.21662714532404034, 0.041443109805523244, -0.037122422346362365, -0.11852788635426098, -0.11915700641219262, 0.174850107256875, 0.17414501785404152, 0.0008317667783962355, 0.039615407655084575, 0.07801318656290984, 0.00034276316701262085, 0.14209066393474737, -0.1313442459364456, 0.021423536904708104, 0.09374339303758461, 0.09616767263246907, -0.2747072940994123, 0.2547538158897724, -0.13737241534895642, 0.14273385106827374, 0.07636415456525153, 0.23763644074400267, -0.14435395269430484, 0.08294159905226142, -0.0853302882711028, 0.25461684243270644, 0.16132017166388254, 0.37117462839793275, -0.15905362419370148, -0.06320917513005918, 0.03640729009553238, -0.058946937115656003], [-0.08800894262917015, 0.05291134313396786, -0.001031180944941614, -0.001358018985585026, 0.257357577347885, -0.17667819276128127, 0.13826534508363061, 0.30312370296324725, 0.33937349632058456, -0.04746664015819197, 0.058709191411490676, 0.1828815269931827, -0.004671136625920949, -0.045105986641315016, -0.17232733772581685, -0.40370217427287414, 0.251109457190108, 0.22168688322214977, -0.00401586907632325, 0.1709922656983785, -0.2055206934557013, -0.1989492265831517, -0.1471007137923785, 0.02827743218158898, 0.05017059456314082, 0.03905628577036702, -0.14681286138036978, 0.14397992809181628, 0.12532531995447757, -0.09744638172180756, -0.15995474157692946, -0.14509179969520672, -0.012345618365899376, 0.16864071408180398, 0.35972045110943524, 0.11299262275038612, 0.07082326920784038, 0.048271064525065216, 0.01122529735363534, 0.04609645706722918, -0.20088123545334066, -0.10408983488931604, 0.24408976135411017, -0.0753248271572849, 0.2342260964923417, 0.16254873445217052, 0.3968799291583507, 0.06536157882731894, -0.26400221381640143, 0.0106198461321385, -0.062146353338428766, -0.04011806744434263, 0.04644785991505436, 0.1950422246262188, -0.15317898436004054, 0.4001139156196428, -0.12310981757068278, 0.16831738662744022, -0.024245930410435667, -0.008083573967704306, 0.1326353281898343, -0.1892247096453186, 0.048732706755602165, -0.12232649743395008, 0.11908492984493142, 0.09327313862740993, 0.1490048477831094, 0.13588442778943674, 0.0492138937925515, -0.3466027763712665, 0.1686541272894196, 0.31771433223848755, 0.1277722475402381, -0.16554449425767298, 0.002214688848218192, -0.01508923976317696, -0.014083898666760197, 0.1531165240170515, 0.12275996344649921, 0.2021434704732636, -0.12509446529388105, 0.0995583699449249, 0.24571293414286946, 0.24947652876701043, -0.05286374242733354, 0.37198249167372327, -0.08757702302714081, 0.1551022251420047, 0.49164098436417786, 0.1082749578534909, -0.09983240910198378, -0.01936160716349664, 0.2239563451351031, 0.018237075181273016, -0.23073641955852509, 0.1387895557867444, 0.19050007040166986, 0.10270924442548952, 0.16428965540683788, 0.04374587827402612, 0.28204030061707547, -0.131487088844828, -0.0050825058766033335, -0.03585947066059579, -0.11393566275744335, -0.41991372600845667, -0.04116959161246601, 0.12118584035045427, -0.043779732092567115, -0.16054598455641256, -0.35778781998416653, -0.1519429914896255, 0.21595447012425764, -0.29840769173334475, -0.22321356511257245, 0.27167322577746666, -0.1303631313064176, -0.13517886561179615, -0.13823688224605893, -0.2899201540681331, 0.286893869223802, 0.3913651930735163, -0.13306989583551235, 0.33088738564401865, 0.07049655399042302, 0.009993540447043337, -0.11264718710647328, -0.37802829115611053, 0.04422135910262232, 0.35749081441241765, -0.2734333735121333, 0.14346097766057306, 0.3146005673536464, -0.2698249470280564, -0.13134046704710825, -0.06354059880275441, -0.2929467905474746, -0.3210010471065407, -0.2792129017736601, -0.5763730176764986, 0.19861054817295593, -0.360130597229885, -0.14807571041519227, 0.2480399632907432, 0.26745152878372563, -0.2481913141015431, 0.1292181225574535, -0.037132896456371425, 0.11556710723949515, 0.15184177442089372, -0.3847797466441989, -0.09185844473540783, 0.0386561323767123, 0.18465270398123917, -0.0307890837404715, 0.08189603423132845, -0.3093578804122365, -0.16037532225575135, 0.007275310183024924, 0.2756661293299302, 0.20712851758276962, -0.07162403906493084, -0.15456787926023421, 0.07753295904916266, -0.3433428294134185, 0.1404496285173556, -0.02448496319677519, 0.23274625675833743, -0.31402845630098297, -0.011162988517595373, -0.12972644036230835, 0.04284216244907483, 0.08191171256096466, -0.24711846681716648, 0.23829402543766343, 0.1768216497932925, 0.160326582075947, -0.15212323953923973, 0.09378464584765227, 0.0239618764380398, -0.014994919441802345, 0.24861509514891583, -0.1229256677603268, -0.041707767254632454, -0.0058991489527018175, -0.033916261533032295, 0.09397217598946198, 0.6169201759216578, -0.08618265358002289, 0.04144872757403747, -0.13779744437045377, 0.07401473467926616, 0.005203802138566971, 0.06479505421188862, 0.15947050607317817, 0.004767658269923666, -0.022723457087641178, 0.021339153142079063, 0.19668958619560883, 0.2021006667662574, -0.12457715023470961, -0.025386933781429314, 0.04828530019554107, -0.15217451536623033, -0.07517250266897937, 0.13629419955870378, -0.1310228281856879, 0.04376755630516488, 0.022673007890420115, -0.03725713152316925, -0.0643770299932879, 0.0005404932181472363, -0.13891653870434864, -0.07738070489595765, -0.0338154088543809, -0.3845075295185265, -0.566255793866256, -0.37853054864251096, 0.14947037074876868, -0.06400577438509335, 0.5341571035754421, -0.0816996002913979, -0.025266063075674618, -0.34221154062644293, 0.06444614570911812, -0.1427157096888708, 0.21375849567677663, -0.00535478556032657, -0.0741762421498804, 0.11641820246839653, 0.19909043844951235, -0.0003992513672489187, 0.03910476171776005, 0.14814666836805965, -0.024183241002585575, 0.08758233937070421, -0.10893745489580475, 0.040134998463580145, -0.004075801728860191, -0.40748158712749893, 0.13355605444951874, 0.07553031198356462, -0.03808736764704403, 0.2336314217308941, 0.016509236241488354, -0.34517407951795537, 0.06268246083155922, -0.07175002344909286, 0.029874912442882425, -0.14091873581966627, 0.1361833056434989, 0.193059210098632, -0.15278948088298025, -0.1003448354161304, -0.03534673172575624, -0.0318515883029803, 0.2939166891510072, 0.30709074583390483, -0.12439382205838742, -0.5270260645117125, 0.016622684976976852, 0.10358831827002375, -0.11095111428395561, -0.45140280768923136, 0.009687192009195038, -0.3287313416924166, -0.0842305116618619, -0.01931386092758697, -0.21772076902182205, 0.12309933665370487, -0.036329142567094255, -0.2315864280352126, 0.002862179206441278, 0.23418921965133885, 0.042644106823465096, 0.1263728151300355, 0.031099300872048607, -0.12086874708209348, -0.06598500016590823, 0.10013633176846348, -0.19250886095687747, 0.17838592105292025, -0.06857584765099961, 0.01032670018566537, -0.043807010368808456, 0.25783910598788085, 0.00916246697306633, 0.1118125636940417, 0.2593483012055715, -0.02713074794282084, -0.1224641285472266, -0.042332338164155575, -0.11962074798572322, 0.32389160994764254, 0.0862525973327296, 0.3809469687226026, -0.18034575094023478, -0.008823930407347887, -0.10731283012453628, -0.14387936711959218], [0.07754889285812776, 0.29371367891629535, -0.3272892062862714, -0.07127596779416005, 0.14676420576870441, -0.011302515864372253, 0.34991977860530216, 0.23658624043067297, 0.2570565526063244, -0.21307099610567093, 0.4213754744268954, 0.1734829687823852, 0.0008892493012050787, -0.3731740117073059, -0.4103080499917269, -0.4201025863488515, 0.020130178580681484, 0.3213359545916319, 0.11804673546672954, 0.2786648003384471, -0.10915311860541503, -0.32810087502002716, -0.2711460907012224, -0.06706312329818805, -0.0972871348882715, -0.08854318285981815, 0.0023440091560284295, -0.06408152480920155, 0.2696824207281073, -0.321295374383529, -0.2802589585383733, -0.3445319527139266, -0.18789272444943586, 0.2717465491344531, 0.3301814889224867, 0.17148727551102638, 0.09664125026514132, -0.07141777283201615, -0.1429383878906568, -0.18687867117114365, 0.18090442459409437, -0.018074120084444683, 0.5170973899463812, -0.37074827899535495, 0.22041017717371383, 0.1138929221779108, 0.4250779841095209, 0.1341875713939468, -0.2842248221859336, 0.09588615627338488, -0.0706404981513818, 0.002374633060147365, -0.07795186216632526, 0.11303813258806865, -0.1565316542983055, 0.6520199828470746, -0.14720312878489494, 0.43555011600255966, -0.04595480300486088, -0.20904956719217202, 0.3175326869143949, -0.1927641881435799, -0.1373894977538536, -0.0775832502792279, 0.08412030898034573, -0.03885383283098539, 0.1086178148786227, 0.19631613170107207, -0.06727113388478756, -0.16271378627667824, -0.0024733940760294595, 0.2392483651638031, 0.21618532203137875, -0.13906571439777812, 0.14895324657360712, 0.18734574100623527, -0.2611463281015555, 0.1471528640637795, -0.09046193212270737, 0.28172474540770054, -0.13678991453101239, 0.08155218077202638, 0.14720727689564228, 0.2369827168683211, -0.12820030376315117, 0.22161800041794777, -0.021593991822252672, 0.077934394183103, 0.3391852068404357, 0.03583410227050384, -0.24759575227896372, -0.35277380732198554, 0.040178123861551285, 0.06975160042444865, -0.16991779084006944, 0.04583023092709482, 0.6362430217365423, -0.1595926389369803, 0.1162339337170124, -0.062280639074742794, 0.40734750404953957, -0.423552917316556, -0.10238031546274821, -0.2681089793331921, 0.1491610910743475, -0.3486502096056938, 0.15210182468096414, -0.020556299326320488, -0.3975983299314976, -0.04752203651393453, -0.7081744944055876, -0.10789370319495599, -0.13640591812630495, -0.32998196470240754, -0.09735721163451672, 0.2989692911505699, -0.24447474970171848, -0.24134092777967453, 0.3697213164220254, -0.2760126118858655, 0.05016768227020899, 0.33186424771944684, 0.018554781985585578, 0.14647877969158193, 0.15808423391232887, 0.2737045818939805, -0.13469426768521467, -0.3330898967881997, -0.02288491507836928, 0.026701970646778744, 0.03768639840806524, -0.09345083187023799, 0.18912782582143942, -0.26806293971215683, -0.10789005147914092, -0.19875528884585947, -0.39787813648581505, -0.443778321146965, -0.2371666735659043, -0.6695256059368452, -0.14170865109190345, -0.220358457416296, -0.07463676761835814, 0.32089877004424733, 0.1942026394729813, -0.4159690144782265, -0.03448456277449926, 0.2418787827094396, -0.06588013966878255, -0.09649566048756242, 0.12117216642946005, -0.28211938589811325, 0.1327655203640461, 0.06444331393383133, -0.05376702112456163, 0.18570647885402045, -0.020955662553509075, 0.004110942284266154, 0.1652497760951519, 0.22127859915296236, 0.25130122403303784, 0.26595772383734584, -0.09546870365738869, 0.2530152089893818, -0.4342669074734052, -0.28248988774915534, -0.13076399138662964, 0.4924879673247536, -0.09247296813797827, 0.13060986374815306, -0.35399252114196617, -0.049880895763635635, 0.013100490594903627, -0.2781727922459443, -0.06576921169956525, -0.10005860744665067, -0.08695748386283715, 0.06602392718195915, -0.17564339376986027, 0.37318040740986663, -0.04335852712392807, 0.14073515372971693, -0.26009143392244977, -0.22845001332461834, -0.0068159814303119974, 0.020701297946895163, 0.19845356978476048, 0.5851276492079099, -0.06748797961821158, 0.0942847893262903, -0.03832207682232062, 0.25865324027836323, -0.21808302278319994, 0.01554280185761551, 0.39414447887490195, 0.14281602948904037, -0.17724204311768213, -0.12109784626712401, 0.25598222638169926, 0.11338214452068011, -0.4078993470563243, -0.040822927529613175, 0.07433106967558463, 0.2447471215079228, -0.32235801514859, 0.11747838122149308, -0.09798741379442315, 0.36737490942080814, 0.12007589040634532, 0.11795425383994977, -0.190294443241631, -0.10276487283408642, -0.26377738546580076, -0.1492527200219532, -0.03480590072770914, -0.2707953577240308, -0.5168698673757414, -0.40757773102571565, 0.3290303262571494, 0.027031078934669495, 0.5048556153972944, 0.18334977142512798, 0.005887816039224465, -0.06628118455410004, 0.28055691346526146, 0.029698037231961887, 0.17166266528268656, -0.2594160499672095, 0.05331645254045725, -0.007137572392821312, -0.11752441401282947, 0.29426688226521946, -0.050996476163466774, 0.31354054684440297, 0.11146473698318005, 0.10107478499412537, -0.07219312729042333, -0.06714988585251074, 0.028448890273769695, -0.26938304242988426, 0.09927421311537425, -0.05202232140194004, -0.2510468742499749, -0.045567890939613186, -0.15088758617639542, -0.432059387365977, -0.046968745067715645, -0.2455521997374793, 0.048215090141942106, -0.04550678220887979, -0.29801731805006665, 0.29941166937351227, 0.2618414191529155, -0.20313409979765615, -0.013844773173332214, -0.40168728803594905, 0.19619412906467915, 0.31643084253785975, -0.35656316046758246, -0.3449082436660926, -0.20843326036507884, 0.053710440173745155, -0.1672266070575764, -0.3097416463618477, 0.03077370673418045, -0.3056855338315169, 0.005542462963300447, 0.15485125966370106, -0.05037671451767286, 0.03509510044629375, 0.10787867999169976, -0.35186863131821156, -0.17210282664746046, 0.36437617025027674, 0.0804399736225605, 0.14951830885062614, -0.2605618977298339, 0.10676668126446505, 0.09518731012940407, 0.03792600054293871, 0.08957633314033349, -0.024834830934802692, 0.19109456365307173, -0.10030949364105861, -0.3257122083256642, 0.045820916847636305, -0.030090245107809704, -0.22770182757327953, -0.10063691297546029, 0.12854113057255745, 0.03334913092354933, 0.10979414234558742, -0.2882160359004047, 0.3200913825227569, -0.09905826983352502, 0.15736842155456543, -0.3118032175116241, -0.14651083325346312, -0.06275424547493458, 0.03794443033014735]]\n"
          ]
        }
      ],
      "source": [
        "# Task 9 construct a vector representation of each tweet as the average of all the word vectors in the tweet\n",
        "# print(model.wv['and'])\n",
        "\n",
        "def calculate_avg_vector(model, tweet):\n",
        "  avg_vec = [0] * 300\n",
        "  count = 0\n",
        "  for sent in tweet.sents:\n",
        "    for token in sent:\n",
        "      if(model.wv.__contains__(token.text)):\n",
        "        count += 1\n",
        "        word_vec = model.wv[token.text]\n",
        "        for i in range(300):\n",
        "          avg_vec[i] += word_vec[i]\n",
        "  for i, val in enumerate(avg_vec):\n",
        "    if(count != 0):\n",
        "      avg_vec[i] = val/count\n",
        "  return avg_vec\n",
        "\n",
        "avg_vector_for_tweets = []\n",
        "for doc in xTrain:\n",
        "  tweet = nlp(doc)\n",
        "  avg_vector_for_tweets.append(calculate_avg_vector(model, tweet))\n",
        "\n",
        "print(avg_vector_for_tweets[:5])\n",
        "\n",
        "# average of all the vectors\n",
        "# avg_vector = []\n",
        "# for doc in enumerate(model.wv):\n",
        "#   for i, vec in enumerate(doc):\n",
        "#     print(vec)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the testing data into an average word2vec model\n",
        "\n",
        "test_model = gensim.models.Word2Vec(tokenize(xTest), window=5, min_count=2, vector_size=300)\n",
        "print(model)\n",
        "test_vectors = []\n",
        "for doc in xTest:\n",
        "  tweet = nlp(doc)\n",
        "  test_vectors.append(calculate_avg_vector(model, tweet))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUcBMd8TWXJH",
        "outputId": "36c9cf70-a746-4558-e0ad-bd0c9ab9d65d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec<vocab=9181, vector_size=300, alpha=0.025>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10 Train logistic regression model on average tweet vectors\n",
        "\n",
        "logReg = trainLogisticRegression(avg_vector_for_tweets, yTrain)\n",
        "print(\"logistic regression model score is: \" + str(logReg.score(test_vectors, yTest)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWt-3DXMR4-M",
        "outputId": "1a1997d5-a74b-4de3-df07-15f4557da0c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logistic regression model score is: 0.6406685236768802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EQlsBhBGR4Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6PTjNN0bFj0"
      },
      "source": [
        "**Question:** Does dense feature representation improve the accuracy of\n",
        "your logistic regression classifier?\n",
        "\n",
        "**Your answer here:**\n",
        "Dense feature representation doesn't improve the accuracy of the logistic regression classifier in this case. The 4000 feature binary multinomial naive bayes has a higher accuracy score by almost 13%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n98qRGseplJf"
      },
      "outputs": [],
      "source": [
        "# Your results should look similar to this: (\"the Accuracy of the trained classifier is: ...\" below)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnB5z-ZoZEa5"
      },
      "source": [
        "**Part 2: PCA Analysis of Shakepeare's Plays.**\n",
        "\n",
        "The file: will_play_text.csv contains lines from William Shakespeare’s plays. The second column of the file contains the name of\n",
        "the play, while the fifth and the sixth contain the name of the character who spoke and what they spoke, respectively. Tokenize\n",
        "and lower case each line in will_play_text.csv using spacy. The file vocab.txt lists the words in the vocabulary. play_genres.csv stores the genres of Shakepeare's plays.\n",
        "\n",
        "**Task 11 & 12 & 13:**\n",
        "\n",
        "11. Create a term-document matrix where each row represents a word in the vocabulary and each column represents\n",
        "a play. Each entry in this matrix represents the number of times a particular word (defined by the row) occurs in a\n",
        "particular play (defined by the column). Use CountVectorizer in sklearn to create the matrix, using the file vocab.txt as\n",
        "input for the vocabulary parameter. From your term-document matrix, use PCA in sklearn to create a 2-dimensional\n",
        "representation of each play. Visualize these representations to see which plays are most similar to each other. Include the\n",
        "visualization in your answer sheet. You can follow the tutorial [here](https://machinelearningmastery.com/develop-word-embeddings-python-gensim/) to create the visualization (look at the \"PCA\" part).\n",
        "\n",
        "12. What plays are similar to each other? Do they match the grouping of Shakespeare’s plays into comedies, histories,\n",
        "and tragedies here?\n",
        "\n",
        "  **Your answer here:**\n",
        "\n",
        "13. Create another term-document matrix where each row represents a word in the vocabulary and each column\n",
        "represents a play, but with TFIDF counts (using TFIDFVectorizer in sklearn and vocab.txt for vocabulary). Use PCA\n",
        "again on these TFIDF term-document matrix and visualize the plays. Include the visualization in your answer sheet.\n",
        "\n",
        "**Hints:** the PCA function in sklearn doesn't work for sparse inputs, try 'TruncatedSVD' instead. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mqw9fcWNheto"
      },
      "outputs": [],
      "source": [
        "# Your visualization should look similar to this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNMZ8AfPbVEY"
      },
      "source": [
        "**Tasks 14 & 15:**\n",
        "14. Create a word-word matrix where each row (and each column) represents a word in the vocabulary (vocab.txt).\n",
        "Each entry in this matrix represents the number of times a particular word (defined by the row) co-occurs with another\n",
        "word (defined by the column) in a sentence (i.e., line in will_play_text.csv). Using the row word vectors, create a representation\n",
        "of a play as the average of all the word vectors in the play. Use these vector representations of plays to compute\n",
        "average pairwise cosine-similarity between plays that are comedies (do not include self-similarities). You can use the\n",
        "grouping of plays in here.\n",
        "\n",
        "15. Using vector representations of plays computed in task 14, compute average pairwise cosine-similarity between\n",
        "plays that are histories, and between plays that are tragedies (do not include self-similarities).\n",
        "\n",
        "Hint: \n",
        "[How to calculate a word-word-co-occurence-matrix with sklearn](https://stackoverflow.com/questions/35562789/how-do-i-calculate-a-word-word-co-occurrence-matrix-with-sklearn)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZgtbXF50S2R"
      },
      "outputs": [],
      "source": [
        "# Your results should look similar to this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjZgaba7dJpG"
      },
      "source": [
        "**Task 16:**\n",
        "\n",
        "16. Use gensim to learn a 100-dimensional word2vec representation of the words in the play (you can use default\n",
        "parameters but with min count=1 so you can learn vector representations of all the words in your data i.e., no need to\n",
        "use vocab.txt in this question). Use the learned word2vec representation to construct vector representations of plays as\n",
        "the average of all the word vectors in the play. Visualize these representations to see which plays are most similar to each other.\n",
        "\n",
        "**Hint:** From now on, since the inputs are no longer sparse, use the PCA function instead of the 'truncatedSVD' one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibHEJCkv1mmz"
      },
      "outputs": [],
      "source": [
        "# Your results should look similar to this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq77-lY7d3is"
      },
      "source": [
        "**Task 17:**\n",
        "\n",
        "17. Construct the vector representation of each character as the average\n",
        "of the representations of all lines that the character spoke (with the gensim-trained representation). Visualize the characters using PCA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_y3I_dn8JlO"
      },
      "outputs": [],
      "source": [
        "# Your results should look similar to this (figure below):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jreF9lpbeeF0"
      },
      "source": [
        "**Task 18:**\n",
        "\n",
        "18. Can you find plays that are central i.e., closest to centroid to each genre? You could do so by visualizing the play representation with PCA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBB9fWyADMhA"
      },
      "outputs": [],
      "source": [
        "# Your results should look similar to this (here's an example of genre \"histories\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07zHEDZqHhBE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}